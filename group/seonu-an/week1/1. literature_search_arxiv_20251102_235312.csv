Index,Source,ID,Title,Authors,Year,Journal,Abstract,DOI,Keywords,Link
1,arXiv,2507.11972v1,Graph Representations for Reading Comprehension Analysis using Large   Language Model and Eye-Tracking Biomarker,"Yuhong Zhang, Jialu Li, Shilai Yang et al.",2025,"arXiv: cs.CL, q-bio.NC","Reading comprehension is a fundamental skill in human cognitive development. With the advancement of Large Language Models (LLMs), there is a growing need to compare how humans and LLMs understand language across different contexts and apply this understanding to functional tasks such as inference, emotion interpretation, and information retrieval. Our previous work used LLMs and human biomarkers to study the reading comprehension process. The results showed that the biomarkers corresponding to ...",,cs.CL; q-bio.NC,https://arxiv.org/abs/2507.11972v1
2,arXiv,2110.03529v1,Using Single-Trial Representational Similarity Analysis with EEG to   track semantic similarity in emotional word processing,Feng Cheng,2021,"arXiv: q-bio.NC, cs.CL",Electroencephalography (EEG) is a powerful non-invasive brain imaging technique with a high temporal resolution that has seen extensive use across multiple areas of cognitive science research. This thesis adapts representational similarity analysis (RSA) to single-trial EEG datasets and introduces its principles to EEG researchers unfamiliar with multivariate analyses. We have two separate aims: 1. we want to explore the effectiveness of single-trial RSA on EEG datasets; 2. we want to utilize si...,,q-bio.NC; cs.CL,https://arxiv.org/abs/2110.03529v1
3,arXiv,2508.20109v1,A Unified Theory of Language,Robert Worden,2025,"arXiv: q-bio.NC, cs.CL, J.3","A unified theory of language combines a Bayesian cognitive linguistic model of language processing, with the proposal that language evolved by sexual selection for the display of intelligence. The theory accounts for the major facts of language, including its speed and expressivity, and data on language diversity, pragmatics, syntax and semantics. The computational element of the theory is based on Construction Grammars. These give an account of the syntax and semantics of the worlds languages, ...",,q-bio.NC; cs.CL; J.3,https://arxiv.org/abs/2508.20109v1
4,arXiv,1802.01830v1,A Neurobiologically Motivated Analysis of Distributional Semantic Models,Akira Utsumi,2018,"arXiv: cs.CL, q-bio.NC","The pervasive use of distributional semantic models or word embeddings in a variety of research fields is due to their remarkable ability to represent the meanings of words for both practical application and cognitive modeling. However, little has been known about what kind of information is encoded in text-based word vectors. This lack of understanding is particularly problematic when word vectors are regarded as a model of semantic representation for abstract concepts. This paper attempts to r...",,cs.CL; q-bio.NC,https://arxiv.org/abs/1802.01830v1
5,arXiv,2407.10376v1,Large Language Model-based FMRI Encoding of Language Functions for   Subjects with Neurocognitive Disorder,"Yuejiao Wang, Xianmin Gong, Lingwei Meng et al.",2024,"arXiv: q-bio.NC, cs.CL","Functional magnetic resonance imaging (fMRI) is essential for developing encoding models that identify functional changes in language-related brain areas of individuals with Neurocognitive Disorders (NCD). While large language model (LLM)-based fMRI encoding has shown promise, existing studies predominantly focus on healthy, young adults, overlooking older NCD populations and cognitive level correlations. This paper explores language-related functional changes in older NCD adults using LLM-based...",,q-bio.NC; cs.CL,https://arxiv.org/abs/2407.10376v1
6,arXiv,2404.14024v2,Exploring neural oscillations during speech perception via surrogate   gradient spiking neural networks,"Alexandre Bittar, Philip N. Garner",2024,"arXiv: cs.CL, q-bio.NC","Understanding cognitive processes in the brain demands sophisticated models capable of replicating neural dynamics at large scales. We present a physiologically inspired speech recognition architecture, compatible and scalable with deep learning frameworks, and demonstrate that end-to-end gradient descent training leads to the emergence of neural oscillations in the central spiking neural network. Significant cross-frequency couplings, indicative of these oscillations, are measured within and ac...",10.3389/fnins.2024.1449181,cs.CL; q-bio.NC,https://arxiv.org/abs/2404.14024v2
7,arXiv,2311.04742v3,Large-scale study of human memory for meaningful narratives,"Antonios Georgiou, Tankut Can, Mikhail Katkov et al.",2023,"arXiv: cs.CL, q-bio.NC","The statistical study of human memory requires large-scale experiments, involving many stimuli conditions and test subjects. While this approach has proven to be quite fruitful for meaningless material such as random lists of words, naturalistic stimuli, like narratives, have until now resisted such a large-scale study, due to the quantity of manual labor required to design and analyze such experiments. In this work, we develop a pipeline that uses large language models (LLMs) both to design nat...",,cs.CL; q-bio.NC,https://arxiv.org/abs/2311.04742v3
8,arXiv,2508.14869v1,The Prompting Brain: Neurocognitive Markers of Expertise in Guiding   Large Language Models,"Hend Al-Khalifa, Raneem Almansour, Layan Abdulrahman Alhuasini et al.",2025,"arXiv: q-bio.NC, cs.CL","Prompt engineering has rapidly emerged as a critical skill for effective interaction with large language models (LLMs). However, the cognitive and neural underpinnings of this expertise remain largely unexplored. This paper presents findings from a cross-sectional pilot fMRI study investigating differences in brain functional connectivity and network activity between experts and intermediate prompt engineers. Our results reveal distinct neural signatures associated with higher prompt engineering...",,q-bio.NC; cs.CL,https://arxiv.org/abs/2508.14869v1
9,arXiv,2503.19586v2,Distinct social-linguistic processing between humans and large   audio-language models: Evidence from model-brain alignment,"Hanlin Wu, Xufeng Duan, Zhenguang Cai",2025,"arXiv: cs.CL, q-bio.NC","Voice-based AI development faces unique challenges in processing both linguistic and paralinguistic information. This study compares how large audio-language models (LALMs) and humans integrate speaker characteristics during speech comprehension, asking whether LALMs process speaker-contextualized language in ways that parallel human cognitive mechanisms. We compared two LALMs' (Qwen2-Audio and Ultravox 0.5) processing patterns with human EEG responses. Using surprisal and entropy metrics from t...",10.18653/v1/2025.cmcl-1.18,cs.CL; q-bio.NC,https://arxiv.org/abs/2503.19586v2
10,arXiv,2505.22563v1,Do Large Language Models Think Like the Brain? Sentence-Level Evidence   from fMRI and Hierarchical Embeddings,"Yu Lei, Xingyang Ge, Yi Zhang et al.",2025,"arXiv: cs.CL, q-bio.NC","Understanding whether large language models (LLMs) and the human brain converge on similar computational principles remains a fundamental and important question in cognitive neuroscience and AI. Do the brain-like patterns observed in LLMs emerge simply from scaling, or do they reflect deeper alignment with the architecture of human language processing? This study focuses on the sentence-level neural mechanisms of language models, systematically investigating how hierarchical representations in L...",,cs.CL; q-bio.NC,https://arxiv.org/abs/2505.22563v1
11,arXiv,2301.10297v1,Large language models can segment narrative events similarly to humans,"Sebastian Michelmann, Manoj Kumar, Kenneth A. Norman et al.",2023,"arXiv: cs.CL, q-bio.NC","Humans perceive discrete events such as ""restaurant visits"" and ""train rides"" in their continuous experience. One important prerequisite for studying human event perception is the ability of researchers to quantify when one event ends and another begins. Typically, this information is derived by aggregating behavioral annotations from several observers. Here we present an alternative computational approach where event boundaries are derived using a large language model, GPT-3, instead of using h...",,cs.CL; q-bio.NC,https://arxiv.org/abs/2301.10297v1
12,arXiv,2308.04941v3,Integrating large language models and active inference to understand eye   movements in reading and dyslexia,"Francesco Donnarumma, Mirco Frosolone, Giovanni Pezzulo",2023,"arXiv: q-bio.NC, cs.CL","We present a novel computational model employing hierarchical active inference to simulate reading and eye movements. The model characterizes linguistic processing as inference over a hierarchical generative model, facilitating predictions and inferences at various levels of granularity, from syllables to sentences. Our approach combines the strengths of large language models for realistic textual predictions and active inference for guiding eye movements to informative textual information, enab...",,q-bio.NC; cs.CL,https://arxiv.org/abs/2308.04941v3
13,arXiv,2505.10948v1,"The Way We Prompt: Conceptual Blending, Neural Dynamics, and   Prompt-Induced Transitions in LLMs",Makoto Sato,2025,"arXiv: cs.CL, q-bio.NC","Large language models (LLMs), inspired by neuroscience, exhibit behaviors that often evoke a sense of personality and intelligence-yet the mechanisms behind these effects remain elusive. Here, we operationalize Conceptual Blending Theory (CBT) as an experimental framework, using prompt-based methods to reveal how LLMs blend and compress meaning. By systematically investigating Prompt-Induced Transitions (PIT) and Prompt-Induced Hallucinations (PIH), we uncover structural parallels and divergence...",,cs.CL; q-bio.NC,https://arxiv.org/abs/2505.10948v1
14,arXiv,2410.00812v2,Generative causal testing to bridge data-driven models and scientific   theories in language neuroscience,"Richard Antonello, Chandan Singh, Shailee Jain et al.",2024,"arXiv: cs.CL, q-bio.NC","Representations from large language models are highly effective at predicting BOLD fMRI responses to language stimuli. However, these representations are largely opaque: it is unclear what features of the language stimulus drive the response in each brain area. We present generative causal testing (GCT), a framework for generating concise explanations of language selectivity in the brain from predictive models and then testing those explanations in follow-up experiments using LLM-generated stimu...",,cs.CL; q-bio.NC,https://arxiv.org/abs/2410.00812v2
15,arXiv,2510.22860v1,Far from the Shallow: Brain-Predictive Reasoning Embedding through   Residual Disentanglement,"Linyang He, Tianjun Zhong, Richard Antonello et al.",2025,"arXiv: cs.CL, q-bio.NC","Understanding how the human brain progresses from processing simple linguistic inputs to performing high-level reasoning is a fundamental challenge in neuroscience. While modern large language models (LLMs) are increasingly used to model neural responses to language, their internal representations are highly ""entangled,"" mixing information about lexicon, syntax, meaning, and reasoning. This entanglement biases conventional brain encoding analyses toward linguistically shallow features (e.g., lex...",,cs.CL; q-bio.NC,https://arxiv.org/abs/2510.22860v1
16,arXiv,2309.04106v2,Meta predictive learning model of languages in neural circuits,"Chan Li, Junbin Qiu, Haiping Huang",2023,"arXiv: cs.CL, q-bio.NC","Large language models based on self-attention mechanisms have achieved astonishing performances not only in natural language itself, but also in a variety of tasks of different nature. However, regarding processing language, our human brain may not operate using the same principle. Then, a debate is established on the connection between brain computation and artificial self-supervision adopted in large language models. One of most influential hypothesis in brain computation is the predictive cod...",10.1103/PhysRevE.109.044309,cs.CL; q-bio.NC,https://arxiv.org/abs/2309.04106v2
17,arXiv,2411.11061v1,Beyond Human-Like Processing: Large Language Models Perform Equivalently   on Forward and Backward Scientific Text,"Xiaoliang Luo, Michael Ramscar, Bradley C. Love",2024,"arXiv: cs.CL, q-bio.NC","The impressive performance of large language models (LLMs) has led to their consideration as models of human language processing. Instead, we suggest that the success of LLMs arises from the flexibility of the transformer learning architecture. To evaluate this conjecture, we trained LLMs on scientific texts that were either in a forward or backward format. Despite backward text being inconsistent with the structure of human languages, we found that LLMs performed equally well in either format o...",,cs.CL; q-bio.NC,https://arxiv.org/abs/2411.11061v1
18,arXiv,2305.13388v2,The neural dynamics of auditory word recognition and integration,"Jon Gauthier, Roger Levy",2023,"arXiv: cs.CL, q-bio.NC","Listeners recognize and integrate words in rapid and noisy everyday speech by combining expectations about upcoming content with incremental sensory evidence. We present a computational model of word recognition which formalizes this perceptual process in Bayesian decision theory. We fit this model to explain scalp EEG signals recorded as subjects passively listened to a fictional story, revealing both the dynamics of the online auditory word recognition process and the neural correlates of the ...",,cs.CL; q-bio.NC,https://arxiv.org/abs/2305.13388v2
19,arXiv,2507.02947v1,The Application of Large Language Models on Major Depressive Disorder   Support Based on African Natural Products,Linyan Zou,2025,"arXiv: cs.CL, q-bio.NC","Major depressive disorder represents one of the most significant global health challenges of the 21st century, affecting millions of people worldwide and creating substantial economic and social burdens. While conventional antidepressant therapies have provided relief for many individuals, their limitations including delayed onset of action, significant side effects, and treatment resistance in a substantial portion of patients have prompted researchers and healthcare providers to explore altern...",,cs.CL; q-bio.NC,https://arxiv.org/abs/2507.02947v1
20,arXiv,2406.19884v2,Investigating the Timescales of Language Processing with EEG and   Language Models,"Davide Turco, Conor Houghton",2024,"arXiv: cs.CL, q-bio.NC","This study explores the temporal dynamics of language processing by examining the alignment between word representations from a pre-trained transformer-based language model, and EEG data. Using a Temporal Response Function (TRF) model, we investigate how neural activity corresponds to model representations across different layers, revealing insights into the interaction between artificial language models and brain responses during language comprehension. Our analysis reveals patterns in TRFs fro...",,cs.CL; q-bio.NC,https://arxiv.org/abs/2406.19884v2
21,arXiv,1703.03442v2,The cognitive roots of regularization in language,"Vanessa Ferdinand, Simon Kirby, Kenny Smith",2017,"arXiv: cs.CL, q-bio.NC","Regularization occurs when the output a learner produces is less variable than the linguistic data they observed. In an artificial language learning experiment, we show that there exist at least two independent sources of regularization bias in cognition: a domain-general source based on cognitive load and a domain-specific source triggered by linguistic stimuli. Both of these factors modulate how frequency information is encoded and produced, but only the production-side modulations result in r...",,cs.CL; q-bio.NC,https://arxiv.org/abs/1703.03442v2
22,arXiv,2502.18318v1,Mapping of Subjective Accounts into Interpreted Clusters (MOSAIC): Topic   Modelling and LLM applied to Stroboscopic Phenomenology,"Romy Beauté, David J. Schwartzman, Guillaume Dumas et al.",2025,"arXiv: cs.CL, q-bio.NC","Stroboscopic light stimulation (SLS) on closed eyes typically induces simple visual hallucinations (VHs), characterised by vivid, geometric and colourful patterns. A dataset of 862 sentences, extracted from 422 open subjective reports, was recently compiled as part of the Dreamachine programme (Collective Act, 2022), an immersive multisensory experience that combines SLS and spatial sound in a collective setting. Although open reports extend the range of reportable phenomenology, their analysis ...",,cs.CL; q-bio.NC,https://arxiv.org/abs/2502.18318v1
23,arXiv,2403.17299v1,Decoding Probing: Revealing Internal Linguistic Structures in Neural   Language Models using Minimal Pairs,"Linyang He, Peili Chen, Ercong Nie et al.",2024,"arXiv: cs.CL, q-bio.NC","Inspired by cognitive neuroscience studies, we introduce a novel `decoding probing' method that uses minimal pairs benchmark (BLiMP) to probe internal linguistic characteristics in neural language models layer by layer. By treating the language model as the `brain' and its representations as `neural activations', we decode grammaticality labels of minimal pairs from the intermediate layers' representations. This approach reveals: 1) Self-supervised language models capture abstract linguistic str...",,cs.CL; q-bio.NC,https://arxiv.org/abs/2403.17299v1
24,arXiv,1108.4297v1,Why is language well-designed for communication? (Commentary on   Christiansen and Chater: 'Language as shaped by the brain'),Jean-Louis Dessalles,2011,"arXiv: cs.CL, q-bio.NC","Selection through iterated learning explains no more than other non-functional accounts, such as universal grammar, why language is so well-designed for communicative efficiency. It does not predict several distinctive features of language like central embedding, large lexicons or the lack of iconicity, that seem to serve communication purposes at the expense of learnability.",,cs.CL; q-bio.NC,https://arxiv.org/abs/1108.4297v1
25,arXiv,2306.15364v1,The Architecture of a Biologically Plausible Language Organ,"Daniel Mitropolsky, Christos H. Papadimitriou",2023,"arXiv: cs.CL, q-bio.NC","We present a simulated biologically plausible language organ, made up of stylized but realistic neurons, synapses, brain areas, plasticity, and a simplified model of sensory perception. We show through experiments that this model succeeds in an important early step in language acquisition: the learning of nouns, verbs, and their meanings, from the grounded input of only a modest number of sentences. Learning in this system is achieved through Hebbian plasticity, and without backpropagation. Our ...",,cs.CL; q-bio.NC,https://arxiv.org/abs/2306.15364v1
