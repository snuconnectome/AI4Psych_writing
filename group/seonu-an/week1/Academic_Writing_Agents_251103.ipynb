{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ÌïôÏà† ÎÖºÎ¨∏ ÏûëÏÑ±ÏùÑ ÏúÑÌïú Multi-Agent ÏãúÏä§ÌÖú\n",
    "\n",
    "## 1. ÏÜåÍ∞ú\n",
    "\n",
    "### 1.1. Í∞úÏöî\n",
    "\n",
    "Ïù¥ ÎÖ∏Ìä∏Î∂ÅÏùÄ **Ïã¨Î¶¨Ìïô ÎÖºÎ¨∏ ÏûëÏÑ±ÏùÑ ÎèïÎäî 7Í∞ÄÏßÄ Ï†ÑÎ¨∏ ÏóêÏù¥Ï†ÑÌä∏**Î•º Íµ¨ÌòÑÌï©ÎãàÎã§:\n",
    "\n",
    "1. **Literature Search Agent** üìö: PubMedÏóêÏÑú Í¥ÄÎ†® ÎÖºÎ¨∏ Í≤ÄÏÉâ Î∞è CSV Ï†ÄÏû•\n",
    "2. **Introduction Writer Agent** ‚úçÔ∏è: Ï¥àÎ°ùÏùÑ Í∏∞Î∞òÏúºÎ°ú ÏÑúÎ°† ÏûëÏÑ±\n",
    "3. **APA 7 Citation Checker Agent** üìù: APA 7Ìåê ÌòïÏãù Í≤ÄÏ¶ù\n",
    "4. **Coherence Checker Agent** üîó: Î¨∏Îß•Í≥º ÎÖºÎ¶¨Ï†Å ÌùêÎ¶Ñ Í≤ÄÏ¶ù\n",
    "5. **Summarizing Agent** üìã: ÏûëÏÑ±Îêú ÎÇ¥Ïö© ÏöîÏïΩ\n",
    "6. **Figure Generation Agent** üìä: Ïó∞Íµ¨ Í∑∏Î¶º ÏÉùÏÑ±\n",
    "7. **Reader Accessibility Agent** üë•: Í∞ÄÎèÖÏÑ± ÌèâÍ∞Ä (ÎåÄÌïô Ïã†ÏûÖÏÉù ÏàòÏ§Ä)\n",
    "\n",
    "### üéØ 1.2 ÌïôÏäµ Î™©Ìëú\n",
    "\n",
    "- ÎÖºÎ¨∏ ÏûëÏÑ± Í≥ºÏ†ïÏùÑ ÏûêÎèôÌôîÌïòÎäî multi-agent ÏãúÏä§ÌÖú Íµ¨Ï∂ï\n",
    "- Í∞Å ÏóêÏù¥Ï†ÑÌä∏Í∞Ä ÌäπÏ†ï Ïó≠Ìï†ÏùÑ ÏàòÌñâÌïòÎèÑÎ°ù ÏÑ§Í≥Ñ\n",
    "- Ïã§Ï†ú ÌïôÏà† ÎÖºÎ¨∏ ÏûëÏÑ±Ïóê ÌôúÏö© Í∞ÄÎä•Ìïú ÎèÑÍµ¨ Ï†úÏûë"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ÌôòÍ≤Ω ÏÑ§Ï†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ÏÇ¨Ïö©Ìï† Î™®Îç∏: gemini-2.5-flash (Provider: gemini)\n",
      "Using Google Generative AI for Gemini\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Dict\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import FunctionDeclaration, Tool\n",
    "from google.generativeai import protos\n",
    "import inspect\n",
    "import PyPDF2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# ÌôòÍ≤Ω Î≥ÄÏàò Î°úÎìú\n",
    "load_dotenv()\n",
    "\n",
    "# API ÏÑ§Ï†ï\n",
    "API_PROVIDER = \"gemini\"\n",
    "MODEL = \"gemini-2.5-flash\"\n",
    "\n",
    "print(f\"‚úÖ ÏÇ¨Ïö©Ìï† Î™®Îç∏: {MODEL} (Provider: {API_PROVIDER})\")\n",
    "\n",
    "# Gemini API Ï¥àÍ∏∞Ìôî\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "print(\"Using Google Generative AI for Gemini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "# ÌÜµÌï© API ÎûòÌçº ÌÅ¥ÎûòÏä§\n",
    "class UnifiedLLMClient:\n",
    "    \"\"\"Gemini APIÎ•º ÏÇ¨Ïö©ÌïòÎäî ÎûòÌçº ÌÅ¥ÎûòÏä§\"\"\"\n",
    "\n",
    "    def __init__(self, provider, model):\n",
    "        self.provider = provider\n",
    "        self.model = model\n",
    "\n",
    "    def _convert_tools_to_gemini(self, tools: List) -> List[Tool]:\n",
    "        \"\"\"Python Ìï®ÏàòÎ•º Gemini Tool ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò\"\"\"\n",
    "        function_declarations = []\n",
    "\n",
    "        for tool in tools:\n",
    "            description = tool.__doc__ or f\"Execute {tool.__name__}\"\n",
    "            description = description.strip()\n",
    "\n",
    "            sig = inspect.signature(tool)\n",
    "            parameters = {\"type\": \"object\", \"properties\": {}, \"required\": []}\n",
    "\n",
    "            for param_name, param in sig.parameters.items():\n",
    "                param_type = \"string\"\n",
    "                if param.annotation != inspect.Parameter.empty:\n",
    "                    if param.annotation == int:\n",
    "                        param_type = \"integer\"\n",
    "                    elif param.annotation == float:\n",
    "                        param_type = \"number\"\n",
    "                    elif param.annotation == bool:\n",
    "                        param_type = \"boolean\"\n",
    "\n",
    "                parameters[\"properties\"][param_name] = {\"type\": param_type}\n",
    "\n",
    "                if param.default == inspect.Parameter.empty:\n",
    "                    parameters[\"required\"].append(param_name)\n",
    "\n",
    "            func_decl = FunctionDeclaration(\n",
    "                name=tool.__name__,\n",
    "                description=description,\n",
    "                parameters=parameters\n",
    "            )\n",
    "            function_declarations.append(func_decl)\n",
    "\n",
    "        return [Tool(function_declarations=function_declarations)]\n",
    "\n",
    "    def chat_completions_create(self, messages, tools, max_turns=5):\n",
    "        \"\"\"ÌÜµÌï© chat completion Ïù∏ÌÑ∞ÌéòÏù¥Ïä§\"\"\"\n",
    "        gemini_model = genai.GenerativeModel(\n",
    "            model_name=self.model,\n",
    "            tools=self._convert_tools_to_gemini(tools)\n",
    "        )\n",
    "\n",
    "        prompt = messages[-1][\"content\"] if isinstance(messages, list) else str(messages)\n",
    "        tool_map = {tool.__name__: tool for tool in tools}\n",
    "\n",
    "        chat = gemini_model.start_chat()\n",
    "        response = chat.send_message(prompt)\n",
    "\n",
    "        # Ìï®Ïàò Ìò∏Ï∂ú Î£®ÌîÑ\n",
    "        for turn in range(max_turns):\n",
    "            function_called = False\n",
    "\n",
    "            for part in response.parts:\n",
    "                if part.function_call:\n",
    "                    function_name = part.function_call.name\n",
    "                    function_args = dict(part.function_call.args)\n",
    "\n",
    "                    function_result = tool_map[function_name](**function_args)\n",
    "\n",
    "                    function_response_part = protos.Part(\n",
    "                        function_response=protos.FunctionResponse(\n",
    "                            name=function_name,\n",
    "                            response={\"result\": str(function_result)}\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    response = chat.send_message(function_response_part)\n",
    "                    function_called = True\n",
    "                    break\n",
    "\n",
    "            if not function_called:\n",
    "                break\n",
    "\n",
    "        class MockChoice:\n",
    "            def __init__(self, text):\n",
    "                self.message = type('obj', (object,), {'content': text})()\n",
    "\n",
    "        class MockResponse:\n",
    "            def __init__(self, text):\n",
    "                self.choices = [MockChoice(text)]\n",
    "\n",
    "        return MockResponse(response.text)\n",
    "\n",
    "# ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî\n",
    "client = UnifiedLLMClient(API_PROVIDER, MODEL)\n",
    "print(\"‚úÖ LLM ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Agent 1: Literature Search Agent üìö\n",
    "\n",
    "**PubMedÏôÄ arXiv**ÏóêÏÑú ÎÖºÎ¨∏ÏùÑ Í≤ÄÏÉâÌïòÍ≥† CSV ÌååÏùºÎ°ú Ï†ÄÏû•ÌïòÎäî ÏóêÏù¥Ï†ÑÌä∏ÏûÖÎãàÎã§.\n",
    "\n",
    "### üìö PubMed Í≤ÄÏÉâ\n",
    "- **Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§**: PubMed API (NCBI E-utilities via Biopython)\n",
    "- **Í∏∞Ïà† Ïä§ÌÉù**: BiopythonÏùò `Bio.Entrez` Î™®Îìà ÏÇ¨Ïö©\n",
    "- **ÌäπÏßï**:\n",
    "  - ÏùòÌïô, ÏÉùÎ™ÖÍ≥ºÌïô, Ïã¨Î¶¨Ìïô ÎÖºÎ¨∏\n",
    "  - MeSH ÌÇ§ÏõåÎìú ÏûêÎèô Ï∂îÏ∂ú\n",
    "  - DOI Î∞è PMID Ìè¨Ìï®\n",
    "  - üö´ Ï†úÏô∏ Îã®Ïñ¥ (Í∏∞Î≥∏Í∞í):\n",
    "    - `psychosis` - Ï†ïÏã†Î≥ë Í¥ÄÎ†® ÎÖºÎ¨∏ Ï†úÏô∏\n",
    "    - `neuro*` - neuroscience, neurology, neuropsychology Îì± neuroÎ°ú ÏãúÏûëÌïòÎäî Î™®Îì† Îã®Ïñ¥ Ï†úÏô∏\n",
    "\n",
    "### üìÑ arXiv Í≤ÄÏÉâ\n",
    "- **Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§**: arXiv API\n",
    "- **Ïπ¥ÌÖåÍ≥†Î¶¨**: cs.CL (Computation and Language), q-bio.NC (Neurons and Cognition)\n",
    "- **ÌäπÏßï**:\n",
    "  - Ïª¥Ìì®ÌÑ∞Í≥ºÌïô + Ïù∏ÏßÄÍ≥ºÌïô Î∂ÑÏïº ÎÖºÎ¨∏\n",
    "  - ÏµúÏã† ÌîÑÎ¶¨ÌîÑÎ¶∞Ìä∏ ÎÖºÎ¨∏ Ìè¨Ìï®\n",
    "  - Ïπ¥ÌÖåÍ≥†Î¶¨ Í∏∞Î∞ò ÌïÑÌÑ∞ÎßÅ\n",
    "\n",
    "### üìä Ï∂úÎ†• ÌòïÏãù\n",
    "CSV ÌååÏùº: `Index, Source, ID, Title, Authors, Year, Journal, Abstract, DOI, Keywords, Link`\n",
    "\n",
    "### üéØ ÏÇ¨Ïö© ÏòàÏãú\n",
    "```python\n",
    "# Í∏∞Î≥∏ ÏÇ¨Ïö© (PubMed + arXiv Î™®Îëê Í≤ÄÏÉâ)\n",
    "literature_search_agent(\n",
    "    topic=\"emotion recognition\",\n",
    "    keywords=\"large language model\",\n",
    "    max_results=10,\n",
    "    email=\"your.email@example.com\"\n",
    ")\n",
    "\n",
    "# PubMedÎßå Í≤ÄÏÉâ (Ï†úÏô∏ Îã®Ïñ¥ Ïª§Ïä§ÌÑ∞ÎßàÏù¥Ï¶à)\n",
    "literature_search_agent(\n",
    "    topic=\"cognitive training\",\n",
    "    keywords=\"working memory aging\",\n",
    "    sources=['pubmed'],\n",
    "    exclude_terms=['psychosis', 'neuro*', 'schizophrenia']\n",
    ")\n",
    "\n",
    "# arXivÎßå Í≤ÄÏÉâ (Îã§Î•∏ Ïπ¥ÌÖåÍ≥†Î¶¨)\n",
    "literature_search_agent(\n",
    "    topic=\"sentiment analysis\",\n",
    "    keywords=\"deep learning\",\n",
    "    sources=['arxiv'],\n",
    "    arxiv_categories=['cs.CL', 'cs.AI']\n",
    ")\n",
    "```\n",
    "\n",
    "### üìù Ï∞∏Í≥†ÏÇ¨Ìï≠\n",
    "- PubMed Í≤ÄÏÉâ Ïãú NCBI Ï†ïÏ±ÖÏóê Îî∞Îùº Ïù¥Î©îÏùº Ï£ºÏÜåÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§\n",
    "- `neuro*`Îäî PubMedÏóêÏÑú neuroÎ°ú ÏãúÏûëÌïòÎäî Î™®Îì† Îã®Ïñ¥Î•º Ï†úÏô∏ÌïòÎäî ÏôÄÏùºÎìúÏπ¥ÎìúÏûÖÎãàÎã§\n",
    "- arXivÎäî ÏÜåÏÜç Ï†ïÎ≥¥Î•º Ï†úÍ≥µÌïòÏßÄ ÏïäÏúºÎØÄÎ°ú Ïπ¥ÌÖåÍ≥†Î¶¨ Í∏∞Î∞òÏúºÎ°úÎßå ÌïÑÌÑ∞ÎßÅÎê©ÎãàÎã§\n",
    "- Îëê Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î™®Îëê Î≥ÑÎèÑÏùò CSV ÌååÏùºÎ°ú Ï†ÄÏû•Îê©ÎãàÎã§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Literature Search Tools Ï†ïÏùò ÏôÑÎ£å (PubMed + arXiv)\n"
     ]
    }
   ],
   "source": [
    "from Bio import Entrez\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import time\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def search_pubmed(topic: str, keywords: str, max_results: int = 10, email: str = \"your.email@example.com\", exclude_terms: list = None) -> str:\n",
    "    \"\"\"\n",
    "    Search for academic papers using PubMed API (via Biopython's Entrez) and save to CSV file.\n",
    "    \n",
    "    This function uses the official NCBI E-utilities API through Biopython.\n",
    "    Google Scholar API is NOT used.\n",
    "    \n",
    "    Args:\n",
    "        topic: Research topic or title\n",
    "        keywords: Psychological constructs or key terms to search\n",
    "        max_results: Maximum number of results to return (default: 10)\n",
    "        email: Email for NCBI API (required by NCBI policy)\n",
    "        exclude_terms: List of terms to exclude from search (default: ['psychosis', 'neuro*'])\n",
    "    \n",
    "    Returns:\n",
    "        Message about the saved CSV file with search results\n",
    "    \"\"\"\n",
    "    # NCBI requires email for API access\n",
    "    Entrez.email = email\n",
    "    \n",
    "    # Í∏∞Î≥∏ Ï†úÏô∏ Îã®Ïñ¥ ÏÑ§Ï†ï (neuro*Îäî neuroÎ°ú ÏãúÏûëÌïòÎäî Î™®Îì† Îã®Ïñ¥ Ï†úÏô∏)\n",
    "    if exclude_terms is None:\n",
    "        exclude_terms = ['psychosis', 'neuro*','Neuro*', 'EEG']\n",
    "    \n",
    "    try:\n",
    "        # Í≤ÄÏÉâ ÏøºÎ¶¨ ÏÉùÏÑ± - PubMedÎäî ANDÎ°ú Îã®Ïñ¥Î•º ÏûêÎèô Ïó∞Í≤∞\n",
    "        base_query = f\"{topic} {keywords}\"\n",
    "        \n",
    "        # Ï†úÏô∏ Îã®Ïñ¥Í∞Ä ÏûàÏúºÎ©¥ NOT Ïó∞ÏÇ∞ÏûêÎ°ú Ï∂îÍ∞Ä\n",
    "        if exclude_terms:\n",
    "            exclude_parts = ' NOT '.join(exclude_terms)\n",
    "            search_query = f\"({base_query}) NOT ({exclude_parts})\"\n",
    "        else:\n",
    "            search_query = base_query\n",
    "        \n",
    "        print(f\"üîç PubMed Í≤ÄÏÉâ Ï§ë: {search_query}\")\n",
    "        \n",
    "        # 1Îã®Í≥Ñ: PubMed ID Í≤ÄÏÉâ\n",
    "        search_handle = Entrez.esearch(\n",
    "            db=\"pubmed\",\n",
    "            term=search_query,\n",
    "            retmax=max_results,\n",
    "            sort=\"relevance\"\n",
    "        )\n",
    "        search_results = Entrez.read(search_handle)\n",
    "        search_handle.close()\n",
    "        \n",
    "        id_list = search_results.get(\"IdList\", [])\n",
    "        \n",
    "        if not id_list:\n",
    "            raise Exception(f\"No papers found in PubMed for query: {search_query}\")\n",
    "        \n",
    "        print(f\"‚úÖ Í≤ÄÏÉâÎêú ÎÖºÎ¨∏ Ïàò: {len(id_list)}\")\n",
    "        if exclude_terms:\n",
    "            print(f\"üö´ Ï†úÏô∏Îêú Îã®Ïñ¥: {', '.join(exclude_terms)}\")\n",
    "        \n",
    "        # 2Îã®Í≥Ñ: ÎÖºÎ¨∏ ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        papers = []\n",
    "        batch_size = 10  # Ìïú Î≤àÏóê Í∞ÄÏ†∏Ïò¨ ÎÖºÎ¨∏ Ïàò\n",
    "        \n",
    "        for i in range(0, len(id_list), batch_size):\n",
    "            batch_ids = id_list[i:i+batch_size]\n",
    "            \n",
    "            print(f\"üì• ÎÖºÎ¨∏ Ï†ïÎ≥¥ ÏàòÏßë Ï§ë: {i+1}-{min(i+batch_size, len(id_list))}/{len(id_list)}\")\n",
    "            \n",
    "            # efetchÎ°ú ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "            fetch_handle = Entrez.efetch(\n",
    "                db=\"pubmed\",\n",
    "                id=batch_ids,\n",
    "                rettype=\"medline\",\n",
    "                retmode=\"xml\"\n",
    "            )\n",
    "            records = Entrez.read(fetch_handle)\n",
    "            fetch_handle.close()\n",
    "            \n",
    "            # Í∞Å ÎÖºÎ¨∏ Ï†ïÎ≥¥ Ï∂îÏ∂ú\n",
    "            for record in records.get('PubmedArticle', []):\n",
    "                paper_info = _extract_pubmed_info(record)\n",
    "                if paper_info:\n",
    "                    papers.append(paper_info)\n",
    "            \n",
    "            time.sleep(0.5)  # API Ìò∏Ï∂ú Ï†úÌïú Ï§ÄÏàò\n",
    "        \n",
    "        if not papers:\n",
    "            raise Exception(\"Could not extract paper information\")\n",
    "        \n",
    "        # CSV ÌååÏùºÎ°ú Ï†ÄÏû•\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"literature_search_pubmed_{timestamp}.csv\"\n",
    "        \n",
    "        with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "            fieldnames = ['Index', 'Source', 'ID', 'Title', 'Authors', 'Year', 'Journal', 'Abstract', 'DOI', 'Keywords', 'Link']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            \n",
    "            writer.writeheader()\n",
    "            for idx, paper in enumerate(papers, 1):\n",
    "                paper['Index'] = idx\n",
    "                paper['Source'] = 'PubMed'\n",
    "                writer.writerow(paper)\n",
    "        \n",
    "        print(f\"üíæ CSV ÌååÏùº Ï†ÄÏû• ÏôÑÎ£å: {filename}\")\n",
    "        \n",
    "        exclude_msg = f\", excluded: {', '.join(exclude_terms)}\" if exclude_terms else \"\"\n",
    "        return f\"Successfully found and saved {len(papers)} papers to '{filename}'{exclude_msg}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå PubMed Í≤ÄÏÉâ Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "def search_arxiv(\n",
    "    topic: str,\n",
    "    keywords: str,\n",
    "    max_results: int = 10,\n",
    "    categories: list = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Search for academic papers using arXiv API and save to CSV file.\n",
    "\n",
    "    Args:\n",
    "        topic: Research topic or title\n",
    "        keywords: Key terms to search\n",
    "        max_results: Maximum number of results to fetch (default: 10)\n",
    "        categories: List of arXiv categories (default: ['cs.CL', 'q-bio.NC'])\n",
    "\n",
    "    Returns:\n",
    "        Message about the saved CSV file with search results\n",
    "    \"\"\"\n",
    "    # Í∏∞Î≥∏ Ïπ¥ÌÖåÍ≥†Î¶¨ ÏÑ§Ï†ï\n",
    "    if categories is None:\n",
    "        categories = ['cs.CL', 'q-bio.NC']\n",
    "\n",
    "    try:\n",
    "        # Í≤ÄÏÉâ ÏøºÎ¶¨ ÏÉùÏÑ±\n",
    "        search_terms = f\"{topic} {keywords}\"\n",
    "\n",
    "        # Ïπ¥ÌÖåÍ≥†Î¶¨ ÌïÑÌÑ∞ Ï∂îÍ∞Ä\n",
    "        category_query = ' OR '.join([f'cat:{cat}' for cat in categories])\n",
    "        full_query = f\"({category_query}) AND ({search_terms})\"\n",
    "\n",
    "        print(f\"üîç arXiv Í≤ÄÏÉâ Ï§ë: {full_query}\")\n",
    "        print(f\"üìÇ Ïπ¥ÌÖåÍ≥†Î¶¨: {', '.join(categories)}\")\n",
    "\n",
    "        # arXiv API Ìò∏Ï∂ú\n",
    "        base_url = 'http://export.arxiv.org/api/query?'\n",
    "        query_params = {\n",
    "            'search_query': full_query,\n",
    "            'start': 0,\n",
    "            'max_results': max_results,\n",
    "            'sortBy': 'relevance',\n",
    "            'sortOrder': 'descending'\n",
    "        }\n",
    "\n",
    "        url = base_url + urllib.parse.urlencode(query_params)\n",
    "\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            xml_data = response.read()\n",
    "\n",
    "        # XML ÌååÏã±\n",
    "        root = ET.fromstring(xml_data)\n",
    "\n",
    "        # Namespace Ï≤òÎ¶¨\n",
    "        ns = {\n",
    "            'atom': 'http://www.w3.org/2005/Atom',\n",
    "            'arxiv': 'http://arxiv.org/schemas/atom'\n",
    "        }\n",
    "\n",
    "        entries = root.findall('atom:entry', ns)\n",
    "\n",
    "        if not entries:\n",
    "            raise Exception(\"No papers found in arXiv\")\n",
    "\n",
    "        print(f\"‚úÖ Í≤ÄÏÉâÎêú ÎÖºÎ¨∏ Ïàò: {len(entries)}\")\n",
    "\n",
    "        # ÎÖºÎ¨∏ Ï†ïÎ≥¥ Ï∂îÏ∂ú\n",
    "        papers = []\n",
    "\n",
    "        for entry in entries:\n",
    "            paper_info = _extract_arxiv_info(entry, ns)\n",
    "            if paper_info:\n",
    "                papers.append(paper_info)\n",
    "\n",
    "        if not papers:\n",
    "            raise Exception(\"Could not extract paper information\")\n",
    "\n",
    "        # CSV ÌååÏùºÎ°ú Ï†ÄÏû•\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"literature_search_arxiv_{timestamp}.csv\"\n",
    "\n",
    "        with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "            fieldnames = ['Index', 'Source', 'ID', 'Title', 'Authors', 'Year', 'Journal', 'Abstract', 'DOI', 'Keywords', 'Link']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            writer.writeheader()\n",
    "            for idx, paper in enumerate(papers, 1):\n",
    "                paper['Index'] = idx\n",
    "                paper['Source'] = 'arXiv'\n",
    "                writer.writerow(paper)\n",
    "\n",
    "        print(f\"üíæ CSV ÌååÏùº Ï†ÄÏû• ÏôÑÎ£å: {filename}\")\n",
    "\n",
    "        return f\"Successfully found and saved {len(papers)} papers to '{filename}'\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå arXiv Í≤ÄÏÉâ Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "def _extract_pubmed_info(record):\n",
    "    \"\"\"PubMed Î†àÏΩîÎìúÏóêÏÑú ÌïÑÏöîÌïú Ï†ïÎ≥¥ Ï∂îÏ∂ú\"\"\"\n",
    "    try:\n",
    "        medline_citation = record.get('MedlineCitation', {})\n",
    "        article = medline_citation.get('Article', {})\n",
    "        \n",
    "        # PMID\n",
    "        pmid = str(medline_citation.get('PMID', ''))\n",
    "        \n",
    "        # Ï†úÎ™©\n",
    "        title = str(article.get('ArticleTitle', 'No title'))\n",
    "        \n",
    "        # Ï†ÄÏûê Ï†ïÎ≥¥\n",
    "        authors = []\n",
    "        author_list = article.get('AuthorList', [])\n",
    "        for author in author_list[:3]:  # ÏµúÎåÄ 3Î™Ö\n",
    "            if 'LastName' in author and 'Initials' in author:\n",
    "                authors.append(f\"{author['LastName']} {author['Initials']}\")\n",
    "        \n",
    "        author_string = ', '.join(authors) if authors else 'No authors'\n",
    "        if len(author_list) > 3:\n",
    "            author_string += ' et al.'\n",
    "        \n",
    "        # Ï∂úÌåê Ïó∞ÎèÑ\n",
    "        pub_date = article.get('Journal', {}).get('JournalIssue', {}).get('PubDate', {})\n",
    "        year = pub_date.get('Year', 'N/A')\n",
    "        \n",
    "        # Ï†ÄÎÑê\n",
    "        journal = str(article.get('Journal', {}).get('Title', 'Unknown journal'))\n",
    "        \n",
    "        # Ï¥àÎ°ù\n",
    "        abstract = \"\"\n",
    "        if 'Abstract' in article:\n",
    "            abstract_parts = article['Abstract'].get('AbstractText', [])\n",
    "            if abstract_parts:\n",
    "                abstract = ' '.join([str(part) for part in abstract_parts])\n",
    "        \n",
    "        # DOI\n",
    "        doi = \"\"\n",
    "        elocation_list = article.get('ELocationID', [])\n",
    "        for eloc in elocation_list:\n",
    "            if hasattr(eloc, 'attributes') and eloc.attributes.get('EIdType') == 'doi':\n",
    "                doi = str(eloc)\n",
    "                break\n",
    "        \n",
    "        # ÌÇ§ÏõåÎìú (MeSH terms)\n",
    "        keywords = []\n",
    "        mesh_list = medline_citation.get('MeshHeadingList', [])\n",
    "        for mesh in mesh_list[:5]:  # ÏµúÎåÄ 5Í∞ú\n",
    "            descriptor = mesh.get('DescriptorName', '')\n",
    "            if descriptor:\n",
    "                keywords.append(str(descriptor))\n",
    "        \n",
    "        keyword_list = medline_citation.get('KeywordList', [])\n",
    "        for kw_group in keyword_list:\n",
    "            for kw in kw_group[:5]:  # ÏµúÎåÄ 5Í∞ú\n",
    "                keywords.append(str(kw))\n",
    "        \n",
    "        keywords_string = '; '.join(keywords[:10]) if keywords else 'No keywords'\n",
    "        \n",
    "        # PubMed ÎßÅÌÅ¨\n",
    "        link = f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/\" if pmid else 'No link'\n",
    "        \n",
    "        return {\n",
    "            'ID': pmid,\n",
    "            'Title': title,\n",
    "            'Authors': author_string,\n",
    "            'Year': year,\n",
    "            'Journal': journal,\n",
    "            'Abstract': abstract[:500] + '...' if len(abstract) > 500 else abstract,\n",
    "            'DOI': doi,\n",
    "            'Keywords': keywords_string,\n",
    "            'Link': link\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è ÎÖºÎ¨∏ Ï†ïÎ≥¥ Ï∂îÏ∂ú Ï§ë Ïò§Î•ò: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def _extract_arxiv_info(entry, ns):\n",
    "    \"\"\"arXiv ÏóîÌä∏Î¶¨ÏóêÏÑú ÌïÑÏöîÌïú Ï†ïÎ≥¥ Ï∂îÏ∂ú\"\"\"\n",
    "    try:\n",
    "        # arXiv ID\n",
    "        arxiv_id = entry.find('atom:id', ns).text.split('/abs/')[-1]\n",
    "        \n",
    "        # Ï†úÎ™©\n",
    "        title = entry.find('atom:title', ns).text.strip().replace('\\n', ' ')\n",
    "        \n",
    "        # Ï†ÄÏûê Ï†ïÎ≥¥\n",
    "        authors = []\n",
    "        \n",
    "        for author in entry.findall('atom:author', ns):\n",
    "            name = author.find('atom:name', ns).text\n",
    "            authors.append(name)\n",
    "        \n",
    "        # Ï†ÄÏûê Î¨∏ÏûêÏó¥ ÏÉùÏÑ± (ÏµúÎåÄ 3Î™Ö)\n",
    "        author_string = ', '.join(authors[:3]) if authors else 'No authors'\n",
    "        if len(authors) > 3:\n",
    "            author_string += ' et al.'\n",
    "        \n",
    "        # Ï∂úÌåêÏùº\n",
    "        published = entry.find('atom:published', ns).text\n",
    "        year = published[:4] if published else 'N/A'\n",
    "        \n",
    "        # Ïπ¥ÌÖåÍ≥†Î¶¨ (Ï†ÄÎÑê ÎåÄÏã†)\n",
    "        categories = []\n",
    "        for cat in entry.findall('atom:category', ns):\n",
    "            categories.append(cat.get('term'))\n",
    "        journal = 'arXiv: ' + ', '.join(categories[:3]) if categories else 'arXiv'\n",
    "        \n",
    "        # Ï¥àÎ°ù\n",
    "        summary = entry.find('atom:summary', ns).text.strip().replace('\\n', ' ')\n",
    "        abstract = summary[:500] + '...' if len(summary) > 500 else summary\n",
    "        \n",
    "        # DOI (ÏûàÎäî Í≤ΩÏö∞)\n",
    "        doi_elem = entry.find('arxiv:doi', ns)\n",
    "        doi = doi_elem.text if doi_elem is not None else ''\n",
    "        \n",
    "        # ÌÇ§ÏõåÎìú (Ïπ¥ÌÖåÍ≥†Î¶¨Î°ú ÎåÄÏ≤¥)\n",
    "        keywords = '; '.join(categories)\n",
    "        \n",
    "        # arXiv ÎßÅÌÅ¨\n",
    "        link = f\"https://arxiv.org/abs/{arxiv_id}\"\n",
    "        \n",
    "        return {\n",
    "            'ID': arxiv_id,\n",
    "            'Title': title,\n",
    "            'Authors': author_string,\n",
    "            'Year': year,\n",
    "            'Journal': journal,\n",
    "            'Abstract': abstract,\n",
    "            'DOI': doi,\n",
    "            'Keywords': keywords,\n",
    "            'Link': link\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è ÎÖºÎ¨∏ Ï†ïÎ≥¥ Ï∂îÏ∂ú Ï§ë Ïò§Î•ò: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"‚úÖ Literature Search Tools Ï†ïÏùò ÏôÑÎ£å (PubMed + arXiv)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Literature Search Agent Ï†ïÏùò ÏôÑÎ£å (PubMed + arXiv)\n"
     ]
    }
   ],
   "source": [
    "def literature_search_agent(\n",
    "    topic: str, \n",
    "    keywords: str, \n",
    "    max_results: int = 50, \n",
    "    email: str = \"your.email@example.com\",\n",
    "    sources: list = None,\n",
    "    exclude_terms: list = None,\n",
    "    arxiv_categories: list = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Search for academic papers from multiple sources and save to CSV.\n",
    "    \n",
    "    This agent searches both PubMed and arXiv databases.\n",
    "    - PubMed: Uses Biopython's Entrez module\n",
    "    - arXiv: Searches specified categories\n",
    "    \n",
    "    Args:\n",
    "        topic: Research topic or title\n",
    "        keywords: Key terms to search\n",
    "        max_results: Maximum number of results per source (default: 10)\n",
    "        email: Email for NCBI API (required by NCBI policy)\n",
    "        sources: List of sources to search (default: ['pubmed', 'arxiv'])\n",
    "        exclude_terms: List of terms to exclude from PubMed search (default: ['psychosis'])\n",
    "        arxiv_categories: List of arXiv categories (default: ['cs.CL', 'q-bio.NC'])\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing agent name and result messages\n",
    "    \"\"\"\n",
    "    # Í∏∞Î≥∏ ÏÜåÏä§ ÏÑ§Ï†ï\n",
    "    if sources is None:\n",
    "        sources = ['pubmed', 'arxiv']\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # PubMed Í≤ÄÏÉâ\n",
    "    if 'pubmed' in sources:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìö PubMed Í≤ÄÏÉâ ÏãúÏûë\")\n",
    "        print(\"=\"*60)\n",
    "        pubmed_result = search_pubmed(topic, keywords, max_results, email, exclude_terms)\n",
    "        results.append(f\"[PubMed] {pubmed_result}\")\n",
    "    \n",
    "    # arXiv Í≤ÄÏÉâ\n",
    "    if 'arxiv' in sources:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìÑ arXiv Í≤ÄÏÉâ ÏãúÏûë\")\n",
    "        print(\"=\"*60)\n",
    "        arxiv_result = search_arxiv(topic, keywords, max_results, arxiv_categories)\n",
    "        results.append(f\"[arXiv] {arxiv_result}\")\n",
    "    \n",
    "    combined_result = \"\\n\\n\".join(results)\n",
    "    \n",
    "    return {\n",
    "        \"agent\": \"Literature Search Agent (PubMed + arXiv)\",\n",
    "        \"result\": combined_result\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Literature Search Agent Ï†ïÏùò ÏôÑÎ£å (PubMed + arXiv)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÌÖåÏä§Ìä∏: Literature Search Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Literature Search Agent ÌÖåÏä§Ìä∏\n",
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üìö PubMed Í≤ÄÏÉâ ÏãúÏûë\n",
      "============================================================\n",
      "üîç PubMed Í≤ÄÏÉâ Ï§ë: (emotion recognition large language model cognitive appraisal empathy) NOT (psychosis NOT neuro* NOT Neuro* NOT EEG)\n",
      "‚ùå PubMed Í≤ÄÏÉâ Ï§ë Ïò§Î•ò Î∞úÏÉù: No papers found in PubMed for query: (emotion recognition large language model cognitive appraisal empathy) NOT (psychosis NOT neuro* NOT Neuro* NOT EEG)\n",
      "\n",
      "============================================================\n",
      "üìÑ arXiv Í≤ÄÏÉâ ÏãúÏûë\n",
      "============================================================\n",
      "üîç arXiv Í≤ÄÏÉâ Ï§ë: (cat:cs.CL OR cat:q-bio.NC) AND (emotion recognition large language model cognitive appraisal empathy)\n",
      "üìÇ Ïπ¥ÌÖåÍ≥†Î¶¨: cs.CL, q-bio.NC\n",
      "‚úÖ Í≤ÄÏÉâÎêú ÎÖºÎ¨∏ Ïàò: 25\n",
      "üíæ CSV ÌååÏùº Ï†ÄÏû• ÏôÑÎ£å: literature_search_arxiv_20251102_235312.csv\n",
      "\n",
      "============================================================\n",
      "Agent: Literature Search Agent (PubMed + arXiv)\n",
      "============================================================\n",
      "[PubMed] Error: No papers found in PubMed for query: (emotion recognition large language model cognitive appraisal empathy) NOT (psychosis NOT neuro* NOT Neuro* NOT EEG)\n",
      "\n",
      "[arXiv] Successfully found and saved 25 papers to 'literature_search_arxiv_20251102_235312.csv'\n"
     ]
    }
   ],
   "source": [
    "# ÏòàÏãú Ïã§Ìñâ\n",
    "print(\"üìö Literature Search Agent ÌÖåÏä§Ìä∏\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ïù¥Î©îÏùº ÏÑ§Ï†ï (Ïã§Ï†ú ÏÇ¨Ïö© Ïãú Î≥∏Ïù∏Ïùò Ïù¥Î©îÏùºÎ°ú Î≥ÄÍ≤Ω)\n",
    "user_email = \"student@snu.ac.kr\"  # NCBI Ï†ïÏ±ÖÏÉÅ ÌïÑÏàò\n",
    "\n",
    "result = literature_search_agent(\n",
    "    topic=\"emotion recognition\",\n",
    "    keywords=\"large language model cognitive appraisal empathy\",  # ÏâºÌëú Ï†úÍ±∞\n",
    "    max_results=50,\n",
    "    email=user_email,\n",
    "    sources=['pubmed', 'arxiv'],  # Îëê ÏÜåÏä§ Î™®Îëê Í≤ÄÏÉâ\n",
    "    arxiv_categories=['cs.CL', 'q-bio.NC']  # cs.CLÍ≥º cognitive science Ïπ¥ÌÖåÍ≥†Î¶¨\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Agent: {result['agent']}\")\n",
    "print(\"=\"*60)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Agent 2: Introduction Writer Agent ‚úçÔ∏è\n",
    "\n",
    "Ï¥àÎ°ùÏùÑ Í∏∞Î∞òÏúºÎ°ú ÏÑúÎ°†ÏùÑ ÏûëÏÑ±ÌïòÎäî ÏóêÏù¥Ï†ÑÌä∏ÏûÖÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Introduction Writer Tools Ï†ïÏùò ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "def analyze_abstract_structure(abstract: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyze the structure and key components of an abstract.\n",
    "    \n",
    "    Args:\n",
    "        abstract: The abstract text to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Analysis of the abstract structure and main components\n",
    "    \"\"\"\n",
    "    # Ï£ºÏöî Íµ¨ÏÑ±ÏöîÏÜå ÌÉêÏßÄ\n",
    "    has_background = any(word in abstract.lower() for word in ['research', 'study', 'literature', 'previous', 'studies show'])\n",
    "    has_purpose = any(word in abstract.lower() for word in ['purpose', 'aim', 'objective', 'examine', 'investigate', 'this study'])\n",
    "    has_method = any(word in abstract.lower() for word in ['method', 'participant', 'design', 'measure', 'procedure'])\n",
    "    has_results = any(word in abstract.lower() for word in ['result', 'found', 'showed', 'demonstrated', 'significant'])\n",
    "    has_conclusion = any(word in abstract.lower() for word in ['suggest', 'conclude', 'implication', 'finding'])\n",
    "    \n",
    "    # Îã®Ïñ¥ Ïàò\n",
    "    word_count = len(abstract.split())\n",
    "    \n",
    "    analysis = \"Abstract Structure Analysis:\\n\"\n",
    "    analysis += f\"- Word count: {word_count}\\n\"\n",
    "    analysis += f\"- Background/Context: {'‚úì Present' if has_background else '‚úó Missing'}\\n\"\n",
    "    analysis += f\"- Research Purpose: {'‚úì Present' if has_purpose else '‚úó Missing'}\\n\"\n",
    "    analysis += f\"- Method: {'‚úì Present' if has_method else '‚úó Missing'}\\n\"\n",
    "    analysis += f\"- Results: {'‚úì Present' if has_results else '‚úó Missing'}\\n\"\n",
    "    analysis += f\"- Conclusion: {'‚úì Present' if has_conclusion else '‚úó Missing'}\\n\\n\"\n",
    "    \n",
    "    # ÌÇ§ÏõåÎìú Ï∂îÏ∂ú (Í∞ÑÎã®Ìïú Î∞©Ïãù)\n",
    "    words = abstract.lower().split()\n",
    "    # Î∂àÏö©Ïñ¥ Ï†úÍ±∞\n",
    "    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'was', 'were', 'this', 'that'}\n",
    "    content_words = [w.strip('.,!?;:()') for w in words if w not in stop_words and len(w) > 4]\n",
    "    \n",
    "    from collections import Counter\n",
    "    word_freq = Counter(content_words)\n",
    "    top_keywords = [word for word, count in word_freq.most_common(10)]\n",
    "    \n",
    "    analysis += f\"Key terms identified: {', '.join(top_keywords[:5])}\\n\"\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def generate_introduction_outline(abstract: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate an outline for the introduction based on the abstract.\n",
    "    \n",
    "    Args:\n",
    "        abstract: The abstract text\n",
    "    \n",
    "    Returns:\n",
    "        A suggested outline for the introduction\n",
    "    \"\"\"\n",
    "    outline = \"Suggested Introduction Outline:\\n\\n\"\n",
    "    outline += \"I. Opening: Broad Context and Importance\\n\"\n",
    "    outline += \"   - Start with the broader topic/issue\\n\"\n",
    "    outline += \"   - Explain why this research area is important\\n\\n\"\n",
    "    \n",
    "    outline += \"II. Literature Review: What We Know\\n\"\n",
    "    outline += \"   - Review existing research on the topic\\n\"\n",
    "    outline += \"   - Highlight key findings from previous studies\\n\"\n",
    "    outline += \"   - Identify patterns or trends in the literature\\n\\n\"\n",
    "    \n",
    "    outline += \"III. Research Gap: What We Don't Know\\n\"\n",
    "    outline += \"   - Point out limitations or gaps in existing research\\n\"\n",
    "    outline += \"   - Explain what remains unclear or unexplored\\n\\n\"\n",
    "    \n",
    "    outline += \"IV. Current Study: What This Study Does\\n\"\n",
    "    outline += \"   - State the purpose/aim of the current study\\n\"\n",
    "    outline += \"   - Present research questions or hypotheses\\n\"\n",
    "    outline += \"   - Briefly mention the approach/method\\n\\n\"\n",
    "    \n",
    "    outline += \"V. Significance: Why This Study Matters\\n\"\n",
    "    outline += \"   - Explain the contribution to the field\\n\"\n",
    "    outline += \"   - Discuss potential implications\\n\\n\"\n",
    "    \n",
    "    return outline\n",
    "\n",
    "def write_introduction_draft(abstract: str, style: str = \"formal\") -> str:\n",
    "    \"\"\"\n",
    "    Write a draft introduction based on the abstract.\n",
    "    \n",
    "    Args:\n",
    "        abstract: The abstract text\n",
    "        style: Writing style (formal, accessible, or concise)\n",
    "    \n",
    "    Returns:\n",
    "        A draft introduction text\n",
    "    \"\"\"\n",
    "    # Ï∂îÏÉÅÏóêÏÑú Ï£ºÏöî Ï†ïÎ≥¥ Ï∂îÏ∂ú\n",
    "    sentences = [s.strip() for s in abstract.split('.') if s.strip()]\n",
    "    \n",
    "    introduction = \"DRAFT INTRODUCTION\\n\"\n",
    "    introduction += \"=\" * 50 + \"\\n\\n\"\n",
    "    \n",
    "    # Opening paragraph (general context)\n",
    "    introduction += \"[PARAGRAPH 1: Opening - Broad Context]\\n\"\n",
    "    introduction += \"In recent years, there has been growing interest in... [Expand on the broader context of your research topic]. \"\n",
    "    introduction += \"This area of research is important because... [Explain significance].\\n\\n\"\n",
    "    \n",
    "    # Literature review\n",
    "    introduction += \"[PARAGRAPH 2-3: Literature Review]\\n\"\n",
    "    introduction += \"Previous research has demonstrated... [Cite relevant studies]. \"\n",
    "    introduction += \"For example, studies have shown that... [Summarize key findings]. \"\n",
    "    introduction += \"Additionally, other researchers have found... [More literature].\\n\\n\"\n",
    "    \n",
    "    # Research gap\n",
    "    introduction += \"[PARAGRAPH 4: Research Gap]\\n\"\n",
    "    introduction += \"However, despite these advances, several questions remain unanswered. \"\n",
    "    introduction += \"Specifically, it is still unclear... [Identify gap]. \"\n",
    "    introduction += \"This gap in knowledge limits our understanding of... [Explain limitation].\\n\\n\"\n",
    "    \n",
    "    # Current study\n",
    "    introduction += \"[PARAGRAPH 5: Current Study]\\n\"\n",
    "    if sentences:\n",
    "        # Ï¥àÎ°ùÏùò Ï≤´ Î¨∏Ïû•ÏùÑ ÌôúÏö©\n",
    "        introduction += f\"The present study addresses this gap by... [Based on: {sentences[0]}]. \"\n",
    "    else:\n",
    "        introduction += \"The present study addresses this gap by... \"\n",
    "    introduction += \"We hypothesize that... [State hypotheses]. \"\n",
    "    introduction += \"To test this, we... [Brief method].\\n\\n\"\n",
    "    \n",
    "    # Significance\n",
    "    introduction += \"[PARAGRAPH 6: Significance]\\n\"\n",
    "    introduction += \"This research contributes to the field by... [Explain contribution]. \"\n",
    "    introduction += \"The findings have important implications for... [Discuss implications].\\n\\n\"\n",
    "    \n",
    "    introduction += \"=\" * 50 + \"\\n\"\n",
    "    introduction += \"Note: This is a template draft. Please:\\n\"\n",
    "    introduction += \"1. Fill in the bracketed sections with specific content\\n\"\n",
    "    introduction += \"2. Add citations from your literature search\\n\"\n",
    "    introduction += \"3. Expand each paragraph with supporting details\\n\"\n",
    "    introduction += \"4. Ensure smooth transitions between paragraphs\\n\"\n",
    "    \n",
    "    return introduction\n",
    "\n",
    "print(\"‚úÖ Introduction Writer Tools Ï†ïÏùò ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File I/O and Introduction Generation Tools Ï†ïÏùò ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "def save_result_to_txt(result: dict, filename: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Save agent result to a txt file.\n",
    "    \n",
    "    Args:\n",
    "        result: Agent result dictionary with 'agent' and 'result' keys\n",
    "        filename: Output filename (auto-generated if None)\n",
    "    \n",
    "    Returns:\n",
    "        Path to saved file\n",
    "    \"\"\"\n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"agent_output_{timestamp}.txt\"\n",
    "    \n",
    "    content = f\"Agent: {result['agent']}\\n\"\n",
    "    content += \"=\"*60 + \"\\n\\n\"\n",
    "    content += result['result']\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    print(f\"üíæ Í≤∞Í≥ºÍ∞Ä '{filename}'Ïóê Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§.\")\n",
    "    return filename\n",
    "\n",
    "def generate_introduction_from_txt(txt_file: str, style: str = \"formal\") -> str:\n",
    "    \"\"\"\n",
    "    Read a txt file and generate an introduction draft using Google API.\n",
    "    \n",
    "    Args:\n",
    "        txt_file: Path to the txt file containing abstract or initial draft\n",
    "        style: Writing style (formal, accessible, or concise)\n",
    "    \n",
    "    Returns:\n",
    "        Generated introduction with APA 7 references\n",
    "    \"\"\"\n",
    "    # Read the txt file\n",
    "    with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Create prompt for Google API\n",
    "    prompt = f\"\"\"\n",
    "You are an expert academic writer specializing in psychology research papers.\n",
    "\n",
    "Task: Based on the following content, write a comprehensive introduction section for a research paper.\n",
    "\n",
    "Content from file:\n",
    "{content}\n",
    "\n",
    "Writing style: {style}\n",
    "\n",
    "IMPORTANT REQUIREMENTS:\n",
    "1. Write a well-structured introduction following the standard academic format:\n",
    "   - Opening with broad context\n",
    "   - Literature review with relevant citations\n",
    "   - Research gap identification\n",
    "   - Current study purpose and hypotheses\n",
    "   - Significance and implications\n",
    "\n",
    "2. Follow the structure and outline provided in the txt file content above. Use it as a guide for organizing your introduction.\n",
    "\n",
    "3. Include in-text citations in APA 7th edition format (Author, Year) throughout the introduction. Do NOT use bold text, asterisks, or any markdown formatting.\n",
    "\n",
    "4. At the end of the introduction, provide a References section with all cited works in APA 7th edition format.\n",
    "\n",
    "5. The introduction should be approximately 2,000 words in length.\n",
    "\n",
    "6. Use plain text only - no bold, italics, or special formatting characters (avoid **, __, etc.).\n",
    "\n",
    "7. Ensure the introduction flows logically and sets up the research effectively.\n",
    "\n",
    "Please generate the introduction draft now with proper APA 7 citations and references.\n",
    "\"\"\"\n",
    "    \n",
    "    # Use Google Gemini API\n",
    "    model = genai.GenerativeModel(model_name=MODEL)\n",
    "    response = model.generate_content(prompt)\n",
    "    \n",
    "    # Save the generated introduction\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_filename = f\"introduction_draft_{timestamp}.txt\"\n",
    "    \n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(\"INTRODUCTION DRAFT (Generated with APA 7 References)\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        f.write(response.text)\n",
    "    \n",
    "    print(f\"‚úÖ Introduction draftÍ∞Ä '{output_filename}'Ïóê Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§.\")\n",
    "    return output_filename\n",
    "\n",
    "print(\"‚úÖ File I/O and Introduction Generation Tools Ï†ïÏùò ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduction_writer_agent(abstract: str, style: str = \"formal\") -> dict:\n",
    "    \"\"\"\n",
    "    Agent specialized in writing introduction sections from abstracts.\n",
    "    \n",
    "    Args:\n",
    "        abstract: The abstract of the research paper\n",
    "        style: Writing style (formal, accessible, or concise)\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are an expert academic writer specializing in psychology research papers.\n",
    "\n",
    "Task: Write a comprehensive introduction section based on the following abstract:\n",
    "\n",
    "{abstract}\n",
    "\n",
    "Writing style: {style}\n",
    "\n",
    "Steps:\n",
    "1. Use analyze_abstract_structure() to understand the abstract components\n",
    "2. Use generate_introduction_outline() to create a structured outline\n",
    "3. Use write_introduction_draft() to generate a draft introduction\n",
    "4. Provide guidance on what sections need more development\n",
    "\n",
    "The introduction should follow the standard structure:\n",
    "- Opening with broad context\n",
    "- Literature review\n",
    "- Research gap identification\n",
    "- Current study purpose\n",
    "- Significance and implications\n",
    "\n",
    "Ensure the introduction is well-organized, flows logically, and sets up the research effectively.\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.chat_completions_create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        tools=[analyze_abstract_structure, generate_introduction_outline, write_introduction_draft],\n",
    "        max_turns=5\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"agent\": \"Introduction Writer Agent\",\n",
    "        \"result\": response.choices[0].message.content\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Introduction Writer Agent Ï†ïÏùò ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úçÔ∏è Introduction Writer Agent ÌÖåÏä§Ìä∏\n",
      "\n",
      "============================================================\n",
      "Îã®Í≥Ñ 1: Ï¥àÎ°ù Î∂ÑÏÑù Î∞è Í≤∞Í≥º Ï†ÄÏû•\n",
      "\n",
      "Agent: Introduction Writer Agent\n",
      "============================================================\n",
      "The draft introduction provides a solid structural foundation, but several sections require significant development to become a comprehensive and well-supported academic introduction.\n",
      "\n",
      "Here is guidance on what sections need more development:\n",
      "\n",
      "1.  **Opening with Broad Context (Paragraph 1):**\n",
      "    *   **Need for Development:** The current draft placeholder, \"In recent years, there has been growing interest in... [Expand on the broader context of your research topic]. This area of research is important because... [Explain significance],\" is very general.\n",
      "    *   **Guidance:** Begin by establishing the profound impact and rapid advancements of Large Language Models (LLMs) in various domains, particularly their emerging capabilities in understanding and processing human language. Connect this to the significance of emotional intelligence and cognitive appraisal within human communication, setting the stage for why LLMs' ability in this area is a critical field of study.\n",
      "\n",
      "2.  **Literature Review (Paragraphs 2-3):**\n",
      "    *   **Need for Development:** These paragraphs are entirely placeholders: \"Previous research has demonstrated... [Cite relevant studies]. For example, studies have shown that... [Summarize key findings]. Additionally, other researchers have found... [More literature].\"\n",
      "    *   **Guidance:** This is a crucial section that needs substantial content.\n",
      "        *   Discuss existing literature on cognitive appraisal theories (e.g., Lazarus, Scherer) and their role in emotion generation.\n",
      "        *   Review prior work on natural language processing (NLP) and emotion detection, highlighting both traditional methods and early applications of LLMs in this area.\n",
      "        *   Specifically, discuss recent findings regarding LLMs' ability to identify cognitive appraisal and emotional outcomes, as well as their use in generating empathetic responses and targeted reappraisals, as mentioned in your abstract. Provide specific examples or types of studies that support these claims.\n",
      "        *   **Crucially, add citations for all claims and studies mentioned.**\n",
      "\n",
      "3.  **Research Gap Identification (Paragraph 4):**\n",
      "    *   **Need for Development:** The placeholder, \"However, despite these advances, several questions remain unanswered. Specifically, it is still unclear... [Identify gap]. This gap in knowledge limits our understanding of... [Explain limitation],\" needs to be specific.\n",
      "    *   **Guidance:** Clearly articulate the specific gap identified in your abstract: \"However, it is unclear which appraisal dimensions they prioritize.\" Elaborate on why this lack of clarity is a significant limitation for advancing LLM capabilities in emotional reasoning and for effective human-AI collaboration.\n",
      "\n",
      "4.  **Current Study Purpose (Paragraph 5):**\n",
      "    *   **Need for Development:** The current placeholder, \"The present study addresses this gap by... [Based on: Large language models (LLMs) are advancing in their ability to identify cognitive appraisal and its corresponding emotional outcome in human-written texts]. We hypothesize that... [State hypotheses]. To test this, we... [Brief method],\" is too vague.\n",
      "    *   **Guidance:** Clearly state the precise objectives of your study based on the abstract. These include:\n",
      "        *   Identifying the author's emotion from seven categories.\n",
      "        *   Explaining the rationale for emotion selection.\n",
      "        *   Rating appraisal intensity from human text.\n",
      "        *   Measuring the accuracy of emotion alignment between models and human annotators.\n",
      "        *   Assessing label agreement of appraisal dimensions between human annotators and models.\n",
      "    *   While the draft includes a section for hypotheses, if your study is primarily exploratory or descriptive in its initial phases, you might focus more on clearly stated research questions rather than formal hypotheses, unless specific predictions are indeed made. Briefly outline the methodological approach (e.g., prompting models, using a specific dataset) as indicated in the abstract.\n",
      "\n",
      "5.  **Significance and Implications (Paragraph 6):**\n",
      "    *   **Need for Development:** The placeholder, \"This research contributes to the field by... [Explain contribution]. The findings have important implications for... [Discuss implications],\" needs to be detailed.\n",
      "    *   **Guidance:** Expand on how your findings contribute to the understanding of LLMs' emotional reasoning capabilities. Explicitly discuss the implications for enhanced human-AI collaboration, the development of more empathetic AI systems, and potential applications in fields such as mental health support or intelligent tutoring. Reiterate the future directions mentioned in the abstract for refining their ability for emotional reasoning.\n",
      "\n",
      "**General Guidance:**\n",
      "*   **Citations:** Integrate relevant citations throughout the literature review and when discussing previous work.\n",
      "*   **Cohesion and Flow:** Ensure smooth logical transitions between paragraphs and ideas. Each paragraph should build upon the previous one.\n",
      "*   **Specificity:** Replace all bracketed placeholders with concrete details from your research.\n",
      "*   **Word Count:** A comprehensive introduction for a research paper is typically substantial, so aim to expand on each point with sufficient detail and evidence.\n",
      "\n",
      "============================================================\n",
      "üíæ Í≤∞Í≥ºÍ∞Ä 'introduction_writer_analysis.txt'Ïóê Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§.\n"
     ]
    }
   ],
   "source": [
    "# ÏòàÏãú Ï¥àÎ°ù\n",
    "sample_abstract = \"\"\"\n",
    "Large language models (LLMs) are advancing in their ability to identify cognitive appraisal and its corresponding emotional outcome in human-written texts. Recent findings show that LLM-based chatbots generate empathetic messages by reasoning about users‚Äô cognitive appraisals and offer targeted reappraisals. However, it is unclear which appraisal dimensions they prioritize. In this work, we prompted the model to (1) identify the author‚Äôs emotion out of seven emotion categories (anger, disgust, fear, guilt, joy, sadness, and shame), (2) explain why that category was selected, and (3) rate appraisal intensity from human text (n=1366). We then measured the accuracy of emotion alignment (i.e., whether the models accurately infer the author's emotion) and label agreement of appraisal dimensions between human annotators from the dataset and the models. We found that GPT4o, GPT-o1, and Gemini-1.5-flash models showed the highest accuracy in identifying joy (average F1 score = .97) and the lowest accuracy in identifying shame (average F1 score = .46), often confusing it with guilt. There was only high agreement (k > .68) in pleasantness and responsibility, whereas the other appraisals showed low agreement (k < .5). We further discuss the implications of LLMs‚Äô ability to understand humans‚Äô emotions for enhanced human-AI collaboration and suggest future directions for refining their ability for emotional reasoning.\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚úçÔ∏è Introduction Writer Agent ÌÖåÏä§Ìä∏\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"Îã®Í≥Ñ 1: Ï¥àÎ°ù Î∂ÑÏÑù Î∞è Í≤∞Í≥º Ï†ÄÏû•\\n\")\n",
    "\n",
    "# Agent Ïã§Ìñâ\n",
    "result = introduction_writer_agent(\n",
    "    abstract=sample_abstract,\n",
    "    style=\"formal\"\n",
    ")\n",
    "\n",
    "print(f\"Agent: {result['agent']}\")\n",
    "print(\"=\"*60)\n",
    "print(result['result'])\n",
    "\n",
    "# Í≤∞Í≥ºÎ•º txt ÌååÏùºÎ°ú Ï†ÄÏû•\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "saved_file = save_result_to_txt(result, \"introduction_writer_analysis.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Îã®Í≥Ñ 2: Google APIÎ°ú Introduction Draft ÏÉùÏÑ± (APA 7 References Ìè¨Ìï®)\n",
      "\n",
      "‚úÖ Introduction draftÍ∞Ä 'introduction_draft_20251103_001826.txt'Ïóê Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§.\n",
      "\n",
      "‚úÖ ÏôÑÎ£å! Îã§Ïùå ÌååÏùºÎì§Ïù¥ ÏÉùÏÑ±ÎêòÏóàÏäµÎãàÎã§:\n",
      "  1. introduction_writer_analysis.txt - Ï¥àÎ°ù Î∂ÑÏÑù Í≤∞Í≥º\n",
      "  2. introduction_draft_20251103_001826.txt - APA 7 referencesÎ•º Ìè¨Ìï®Ìïú ÏôÑÏ†ÑÌïú introduction draft\n"
     ]
    }
   ],
   "source": [
    "# Îã®Í≥Ñ 2: txt ÌååÏùºÏùÑ ÏùΩÏñ¥ÏÑú Google APIÎ°ú ÏôÑÏ†ÑÌïú introduction ÏÉùÏÑ±\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Îã®Í≥Ñ 2: Google APIÎ°ú Introduction Draft ÏÉùÏÑ± (APA 7 References Ìè¨Ìï®)\\n\")\n",
    "\n",
    "introduction_file = generate_introduction_from_txt(\n",
    "    txt_file=saved_file,\n",
    "    style=\"formal\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ ÏôÑÎ£å! Îã§Ïùå ÌååÏùºÎì§Ïù¥ ÏÉùÏÑ±ÎêòÏóàÏäµÎãàÎã§:\")\n",
    "print(f\"  1. {saved_file} - Ï¥àÎ°ù Î∂ÑÏÑù Í≤∞Í≥º\")\n",
    "print(f\"  2. {introduction_file} - APA 7 referencesÎ•º Ìè¨Ìï®Ìïú ÏôÑÏ†ÑÌïú introduction draft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Agent 3: APA 7 Citation Checker Agent üìù\n",
    "APA 7Ìåê ÌòïÏãùÏùÑ ÌôïÏù∏ÌïòÎäî ÏóêÏù¥Ï†ÑÌä∏ÏûÖÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ APA Citation Checker Tools Ï†ïÏùò ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "def extract_apa_guidelines(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract APA 7 citation guidelines from the PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the APA 7 Style PDF file\n",
    "    \n",
    "    Returns:\n",
    "        Extracted text content from the PDF\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            # Ï≤òÏùå 20ÌéòÏù¥ÏßÄÎßå Ï∂îÏ∂ú (Ï£ºÏöî citation Í∑úÏπôÏù¥ ÏûàÎäî Î∂ÄÎ∂Ñ)\n",
    "            for page_num in range(min(20, len(pdf_reader.pages))):\n",
    "                text += pdf_reader.pages[page_num].extract_text()\n",
    "        \n",
    "        return f\"APA 7 Guidelines (excerpt):\\n{text[:3000]}...\"\n",
    "    except Exception as e:\n",
    "        return f\"Error reading PDF: {str(e)}. Using general APA 7 knowledge instead.\"\n",
    "\n",
    "def check_citations_in_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Check for in-text citations in the provided text.\n",
    "    \n",
    "    Args:\n",
    "        text: The manuscript text to check\n",
    "    \n",
    "    Returns:\n",
    "        List of citations found in the text\n",
    "    \"\"\"\n",
    "    # APA ÌòïÏãù Ïù∏Ïö© Ìå®ÌÑ¥: (Author, Year) ÎòêÎäî (Author et al., Year)\n",
    "    citation_pattern = r'\\([A-Z][a-z]+(?:\\s+(?:et al\\.|&|and)\\s+[A-Z][a-z]+)?,\\s*\\d{4}[a-z]?\\)'\n",
    "    citations = re.findall(citation_pattern, text)\n",
    "    \n",
    "    if citations:\n",
    "        unique_citations = list(set(citations))\n",
    "        result = f\"Found {len(unique_citations)} unique citations:\\n\"\n",
    "        for i, cite in enumerate(unique_citations, 1):\n",
    "            result += f\"{i}. {cite}\\n\"\n",
    "        return result\n",
    "    else:\n",
    "        return \"No citations found in the text.\"\n",
    "\n",
    "def check_reference_format(references: str) -> str:\n",
    "    \"\"\"\n",
    "    Check if reference entries follow APA 7 format.\n",
    "    \n",
    "    Args:\n",
    "        references: The reference list to check\n",
    "    \n",
    "    Returns:\n",
    "        Analysis of reference formatting issues\n",
    "    \"\"\"\n",
    "    issues = []\n",
    "    lines = references.split('\\n')\n",
    "    \n",
    "    for i, line in enumerate(lines, 1):\n",
    "        if line.strip():\n",
    "            # Í∏∞Î≥∏ Ï≤¥ÌÅ¨: Ï†ÄÏûê Ïù¥Î¶ÑÏù¥ ÏûàÎäîÏßÄ\n",
    "            if not re.search(r'[A-Z][a-z]+,\\s*[A-Z]', line):\n",
    "                issues.append(f\"Line {i}: Missing proper author format (Last, F. I.)\")\n",
    "            \n",
    "            # Ïó∞ÎèÑÍ∞Ä Í¥ÑÌò∏ ÏïàÏóê ÏûàÎäîÏßÄ\n",
    "            if not re.search(r'\\(\\d{4}[a-z]?\\)', line):\n",
    "                issues.append(f\"Line {i}: Missing or incorrect year format (YYYY)\")\n",
    "    \n",
    "    if issues:\n",
    "        return \"Reference format issues found:\\n\" + \"\\n\".join(issues[:10])  # ÏµúÎåÄ 10Í∞úÎßå ÌëúÏãú\n",
    "    else:\n",
    "        return \"Reference format appears correct.\"\n",
    "\n",
    "def check_citation_reference_match(text: str, references: str) -> str:\n",
    "    \"\"\"\n",
    "    Check if all in-text citations have corresponding references.\n",
    "    \n",
    "    Args:\n",
    "        text: The manuscript text\n",
    "        references: The reference list\n",
    "    \n",
    "    Returns:\n",
    "        Report on citation-reference matching\n",
    "    \"\"\"\n",
    "    # Î≥∏Î¨∏Ïùò Ïù∏Ïö© Ï∂îÏ∂ú\n",
    "    citation_pattern = r'\\(([A-Z][a-z]+(?:\\s+(?:et al\\.|&|and)\\s+[A-Z][a-z]+)?),\\s*(\\d{4}[a-z]?)\\)'\n",
    "    citations = re.findall(citation_pattern, text)\n",
    "    \n",
    "    missing_refs = []\n",
    "    for author, year in citations:\n",
    "        # Ï∞∏Í≥†Î¨∏ÌóåÏóê Ìï¥Îãπ Ï†ÄÏûêÏôÄ Ïó∞ÎèÑÍ∞Ä ÏûàÎäîÏßÄ ÌôïÏù∏\n",
    "        if author not in references or year not in references:\n",
    "            missing_refs.append(f\"{author}, {year}\")\n",
    "    \n",
    "    if missing_refs:\n",
    "        unique_missing = list(set(missing_refs))\n",
    "        return f\"‚ö†Ô∏è Missing references for {len(unique_missing)} citations:\\n\" + \"\\n\".join(unique_missing[:10])\n",
    "    else:\n",
    "        return \"‚úì All citations have corresponding references.\"\n",
    "\n",
    "print(\"‚úÖ APA Citation Checker Tools Ï†ïÏùò ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ APA Citation Checker Agent Ï†ïÏùò ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "def apa_citation_checker_agent(manuscript_text: str, references: str, pdf_path: str = \"HW6/APA7-Style.pdf\") -> dict:\n",
    "    \"\"\"\n",
    "    Agent specialized in checking APA 7 citation format.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are an APA 7th edition style expert.\n",
    "\n",
    "Task: Check the following manuscript for APA 7 compliance:\n",
    "\n",
    "Manuscript Text:\n",
    "{manuscript_text[:2000]}...\n",
    "\n",
    "References:\n",
    "{references}\n",
    "\n",
    "Steps:\n",
    "1. Use check_citations_in_text() to find all citations\n",
    "2. Use check_reference_format() to verify reference formatting\n",
    "3. Use check_citation_reference_match() to ensure all citations have references\n",
    "4. Provide a detailed report with specific issues and corrections needed\n",
    "\n",
    "Focus on accuracy and provide actionable feedback.\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.chat_completions_create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        tools=[check_citations_in_text, check_reference_format, check_citation_reference_match],\n",
    "        max_turns=5\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"agent\": \"APA 7 Citation Checker Agent\",\n",
    "        \"result\": response.choices[0].message.content\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ APA Citation Checker Agent Ï†ïÏùò ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÌÖåÏä§Ìä∏: APA Citation Checker Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ ÏùΩÏùÑ ÌååÏùº: introduction_draft_20251103_001826.txt\n",
      "\n",
      "‚úÖ Î≥∏Î¨∏ Í∏∏Ïù¥: 15499 Ïûê\n",
      "‚úÖ Ï∞∏Í≥†Î¨∏Ìóå Í∏∏Ïù¥: 3481 Ïûê\n",
      "\n",
      "üìù APA Citation Checker Agent ÌÖåÏä§Ìä∏\n",
      "\n",
      "============================================================\n",
      "Agent: APA 7 Citation Checker Agent\n",
      "============================================================\n",
      "Here is a detailed report on the APA 7th edition compliance of your manuscript:\n",
      "\n",
      "**APA 7th Edition Compliance Report**\n",
      "\n",
      "**1. General Manuscript Formatting**\n",
      "\n",
      "*   **Issue:** The heading \"INTRODUCTION DRAFT (Generated with APA 7 ref)\" is not compliant with APA 7th edition guidelines.\n",
      "*   **Correction:** The first heading in a manuscript (usually \"Introduction\") should be centered, bolded, and title case. Any additional descriptive text like \"(Generated with APA 7 ref)\" should be removed.\n",
      "    *   **Example Correction:** **Introduction**\n",
      "\n",
      "**2. In-Text Citations**\n",
      "\n",
      "*   **Status:** All in-text citations appear to have a corresponding entry in your reference list.\n",
      "*   **Details:** The following citations were identified in your text:\n",
      "    *   (Brown et al., 2020)\n",
      "    *   (OpenAI, 2023)\n",
      "    *   (Picard, 1997)\n",
      "    *   (Salovey & Mayer, 1990)\n",
      "    *   (Lazarus, 1991)\n",
      "    *   *Note: My automated tool initially indicated a missing reference for \"Salovey & Mayer, 1990\", but upon manual review, a matching reference was found in your reference list. All cited works appear to have a corresponding entry.*\n",
      "\n",
      "**3. Reference List Formatting**\n",
      "\n",
      "*   **Status:** One formatting issue was identified in your reference list.\n",
      "*   **Details:**\n",
      "    *   **Issue:** The reference for \"Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language Models are Unsupervised Multitask Learners. *OpenAI Blog, 1*(8), 9.\" is missing the proper author format (Last Name, First Initial.). The current format \"Radford, A.\" is correct for the first author, but for subsequent authors, only the first initial should be used.\n",
      "    *   **Correction:** For subsequent authors, ensure they are formatted as \"Initial. Last Name\".\n",
      "        *   **Example Correction:** Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language Models are Unsupervised Multitask Learners. *OpenAI Blog, 1*(8), 9.\n",
      "    *   *Note: While the tool flagged a specific formatting issue, it's always good practice to review all references against APA 7th edition guidelines for complete accuracy, including capitalization of titles, journal names, and volume/issue numbers.*\n",
      "\n",
      "**Summary of Actionable Feedback:**\n",
      "\n",
      "1.  **Revise the main heading** to be \"Introduction\" (centered, bolded).\n",
      "2.  **Correct the author formatting** in the \"Radford et al. (2019)\" reference.\n",
      "\n",
      "Your manuscript demonstrates a good adherence to APA 7th edition style, with only minor adjustments needed.\n"
     ]
    }
   ],
   "source": [
    "# Introduction ÌååÏùº ÏùΩÍ∏∞ Î∞è Î∂ÑÎ¶¨\n",
    "import os\n",
    "\n",
    "# introduction_draft ÌååÏùº Í≤ΩÎ°ú ÏßÄÏ†ï (ÌååÏùºÎ™ÖÏùÑ ÏßÅÏ†ë ÏûÖÎ†•ÌïòÏÑ∏Ïöî)\n",
    "introduction_file = \"introduction_draft_20251103_001826.txt\"  # Ïã§Ï†ú ÌååÏùºÎ™ÖÏúºÎ°ú Î≥ÄÍ≤ΩÌïòÏÑ∏Ïöî\n",
    "\n",
    "# ÌååÏùºÏù¥ Ï°¥Ïû¨ÌïòÎäîÏßÄ ÌôïÏù∏\n",
    "if os.path.exists(introduction_file):\n",
    "    print(f\"üìÑ ÏùΩÏùÑ ÌååÏùº: {introduction_file}\\n\")\n",
    "    \n",
    "    # ÌååÏùº ÏùΩÍ∏∞\n",
    "    with open(introduction_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # \"References\" ÎòêÎäî \"REFERENCES\" Í∏∞Ï§ÄÏúºÎ°ú Î∂ÑÎ¶¨\n",
    "    if 'References' in content:\n",
    "        split_point = content.find('References')\n",
    "    elif 'REFERENCES' in content:\n",
    "        split_point = content.find('REFERENCES')\n",
    "    elif 'Reference' in content:\n",
    "        split_point = content.find('Reference')\n",
    "    else:\n",
    "        # References ÏÑπÏÖòÏù¥ ÏóÜÏúºÎ©¥ Ï†ÑÏ≤¥Î•º Î≥∏Î¨∏ÏúºÎ°ú\n",
    "        split_point = len(content)\n",
    "        print(\"‚ö†Ô∏è References ÏÑπÏÖòÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. Ï†ÑÏ≤¥Î•º Î≥∏Î¨∏ÏúºÎ°ú Ï≤òÎ¶¨Ìï©ÎãàÎã§.\\n\")\n",
    "    \n",
    "    # Î≥∏Î¨∏Í≥º Ï∞∏Í≥†Î¨∏Ìóå Î∂ÑÎ¶¨\n",
    "    sample_text = content[:split_point].strip()\n",
    "    sample_references = content[split_point:].strip() if split_point < len(content) else \"\"\n",
    "    \n",
    "    print(f\"‚úÖ Î≥∏Î¨∏ Í∏∏Ïù¥: {len(sample_text)} Ïûê\")\n",
    "    print(f\"‚úÖ Ï∞∏Í≥†Î¨∏Ìóå Í∏∏Ïù¥: {len(sample_references)} Ïûê\\n\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {introduction_file}\")\n",
    "    print(\"‚ö†Ô∏è ÏòàÏãú Îç∞Ïù¥ÌÑ∞Î•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.\\n\")\n",
    "    \n",
    "    # ÏòàÏãú Îç∞Ïù¥ÌÑ∞\n",
    "    sample_text = \"\"\"\n",
    "Recent studies have shown that cognitive training can improve working memory in older adults \n",
    "(Smith et al., 2023). This finding is consistent with previous research (Johnson & Lee, 2022).\n",
    "However, some studies have found no significant effects (Brown, 2021).\n",
    "\"\"\"\n",
    "    \n",
    "    sample_references = \"\"\"\n",
    "Brown, A. (2021). Cognitive training effects in aging. Journal of Gerontology, 45(3), 234-245.\n",
    "Johnson, M., & Lee, S. (2022). Working memory interventions. Psychological Science, 33(2), 112-125.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìù APA Citation Checker Agent ÌÖåÏä§Ìä∏\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result = apa_citation_checker_agent(sample_text, sample_references)\n",
    "\n",
    "print(f\"Agent: {result['agent']}\")\n",
    "print(\"=\"*60)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Agent 4: Coherence Checker Agent üîó\n",
    "\n",
    "Î¨∏Îß•Ïùò ÎÖºÎ¶¨Ï†Å ÌùêÎ¶ÑÍ≥º ÏùºÍ¥ÄÏÑ±ÏùÑ ÌôïÏù∏ÌïòÎäî ÏóêÏù¥Ï†ÑÌä∏ÏûÖÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Coherence Checker Tools Ï†ïÏùò ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "def check_cohesion(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Check the cohesion of the text using transition words and logical connectors.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Analysis of cohesion quality\n",
    "    \"\"\"\n",
    "    # Ïó∞Í≤∞Ïñ¥/Ï†ÑÌôòÏñ¥ Î™©Î°ù\n",
    "    transition_words = [\n",
    "        'however', 'therefore', 'moreover', 'furthermore', 'nevertheless',\n",
    "        'consequently', 'additionally', 'similarly', 'in contrast', 'for example',\n",
    "        'specifically', 'particularly', 'notably', 'indeed', 'thus'\n",
    "    ]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    found_transitions = [word for word in transition_words if word in text_lower]\n",
    "    \n",
    "    # Î¨∏Ïû• Ïàò Í≥ÑÏÇ∞\n",
    "    sentences = text.split('.')\n",
    "    sentence_count = len([s for s in sentences if s.strip()])\n",
    "    \n",
    "    transition_ratio = len(found_transitions) / max(sentence_count, 1)\n",
    "    \n",
    "    analysis = f\"Cohesion Analysis:\\n\"\n",
    "    analysis += f\"- Sentences: {sentence_count}\\n\"\n",
    "    analysis += f\"- Transition words found: {len(found_transitions)}\\n\"\n",
    "    analysis += f\"- Transition ratio: {transition_ratio:.2f}\\n\"\n",
    "    analysis += f\"- Transitions used: {', '.join(found_transitions[:10])}\\n\\n\"\n",
    "    \n",
    "    if transition_ratio < 0.1:\n",
    "        analysis += \"‚ö†Ô∏è Low cohesion: Consider adding more transition words to improve flow.\"\n",
    "    elif transition_ratio > 0.3:\n",
    "        analysis += \"‚ö†Ô∏è High cohesion: May be using too many transition words.\"\n",
    "    else:\n",
    "        analysis += \"‚úì Good cohesion: Appropriate use of transition words.\"\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def check_comprehension(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Check the comprehension level using readability metrics.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Readability analysis\n",
    "    \"\"\"\n",
    "    # Í∞ÑÎã®Ìïú Í∞ÄÎèÖÏÑ± ÏßÄÌëú Í≥ÑÏÇ∞\n",
    "    words = text.split()\n",
    "    sentences = [s for s in text.split('.') if s.strip()]\n",
    "    \n",
    "    if not words or not sentences:\n",
    "        return \"Not enough text to analyze.\"\n",
    "    \n",
    "    avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "    avg_sentence_length = len(words) / len(sentences)\n",
    "    \n",
    "    # Í∏¥ Îã®Ïñ¥ ÎπÑÏú® (7Ïûê Ïù¥ÏÉÅ)\n",
    "    long_words = [w for w in words if len(w) > 7]\n",
    "    long_word_ratio = len(long_words) / len(words)\n",
    "    \n",
    "    analysis = f\"Comprehension Analysis:\\n\"\n",
    "    analysis += f\"- Total words: {len(words)}\\n\"\n",
    "    analysis += f\"- Avg word length: {avg_word_length:.1f} characters\\n\"\n",
    "    analysis += f\"- Avg sentence length: {avg_sentence_length:.1f} words\\n\"\n",
    "    analysis += f\"- Long words (>7 chars): {len(long_words)} ({long_word_ratio:.1%})\\n\\n\"\n",
    "    \n",
    "    # Í∞ÄÎèÖÏÑ± ÌåêÎã®\n",
    "    if avg_sentence_length > 25:\n",
    "        analysis += \"‚ö†Ô∏è Sentences are too long. Consider breaking them down.\\n\"\n",
    "    if long_word_ratio > 0.3:\n",
    "        analysis += \"‚ö†Ô∏è High proportion of complex words. Simplify vocabulary if possible.\\n\"\n",
    "    if avg_sentence_length < 25 and long_word_ratio < 0.3:\n",
    "        analysis += \"‚úì Good readability level.\\n\"\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def check_vocabulary(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Check for appropriate vocabulary usage and repetition.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Vocabulary analysis\n",
    "    \"\"\"\n",
    "    words = [w.lower().strip('.,!?;:') for w in text.split()]\n",
    "    \n",
    "    # Îã®Ïñ¥ ÎπàÎèÑ Í≥ÑÏÇ∞\n",
    "    from collections import Counter\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    # Î∂àÏö©Ïñ¥ Ï†úÍ±∞\n",
    "    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'was', 'are', 'were'}\n",
    "    content_words = {w: c for w, c in word_freq.items() if w not in stop_words and len(w) > 3}\n",
    "    \n",
    "    # Í∞ÄÏû• ÏûêÏ£º ÏÇ¨Ïö©Îêú Îã®Ïñ¥\n",
    "    top_words = sorted(content_words.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    # Î∞òÎ≥µ ÌôïÏù∏ (3Ìöå Ïù¥ÏÉÅ)\n",
    "    repeated = [(w, c) for w, c in top_words if c >= 3]\n",
    "    \n",
    "    analysis = f\"Vocabulary Analysis:\\n\"\n",
    "    analysis += f\"- Unique words: {len(set(words))}\\n\"\n",
    "    analysis += f\"- Lexical diversity: {len(set(words)) / len(words):.2f}\\n\"\n",
    "    analysis += f\"- Most frequent words: {', '.join([w for w, c in top_words[:5]])}\\n\\n\"\n",
    "    \n",
    "    if repeated:\n",
    "        analysis += f\"‚ö†Ô∏è Frequently repeated words (consider synonyms):\\n\"\n",
    "        for word, count in repeated[:5]:\n",
    "            analysis += f\"  - '{word}': {count} times\\n\"\n",
    "    else:\n",
    "        analysis += \"‚úì Good vocabulary variety.\\n\"\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "print(\"‚úÖ Coherence Checker Tools Ï†ïÏùò ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Coherence Checker Agent Ï†ïÏùò ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "def coherence_checker_agent(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Agent specialized in checking text coherence and comprehension.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a writing quality expert specializing in academic prose.\n",
    "\n",
    "Task: Analyze the following text for coherence, comprehension, and vocabulary:\n",
    "\n",
    "{text}\n",
    "\n",
    "Steps:\n",
    "1. Use check_cohesion() to analyze logical flow\n",
    "2. Use check_comprehension() to assess readability\n",
    "3. Use check_vocabulary() to check word choice\n",
    "4. Provide specific recommendations for improvement\n",
    "\n",
    "Focus on helping improve the clarity and flow of academic writing.\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.chat_completions_create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        tools=[check_cohesion, check_comprehension, check_vocabulary],\n",
    "        max_turns=5\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"agent\": \"Coherence Checker Agent\",\n",
    "        \"result\": response.choices[0].message.content\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Coherence Checker Agent Ï†ïÏùò ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÌÖåÏä§Ìä∏: Coherence Checker Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ ÏùΩÏùÑ ÌååÏùº: introduction_draft_20251103_001826.txt\n",
      "\n",
      "üîó Coherence Checker Agent ÌÖåÏä§Ìä∏\n",
      "\n",
      "============================================================\n",
      "Agent: Coherence Checker Agent\n",
      "============================================================\n",
      "Here's an analysis of your introduction draft, focusing on cohesion, comprehension, and vocabulary, along with recommendations for improvement:\n",
      "\n",
      "**Cohesion Analysis:**\n",
      "\n",
      "*   **Result:** The cohesion analysis indicates a low transition ratio (0.08) with only 8 transition words found across 100 sentences.\n",
      "*   **Recommendation:** To improve the logical flow and connectedness of your ideas, consider incorporating more transition words and phrases. While you use some effective transitions like \"however,\" \"therefore,\" and \"consequently,\" more explicit linking between sentences and paragraphs would enhance the reader's ability to follow your arguments. Examples of transitions that could be used more frequently include: \"In addition,\" \"Furthermore,\" \"Moreover,\" \"Conversely,\" \"For instance,\" \"As a result,\" etc. Also, ensure that topic sentences clearly signal the relationship of each paragraph to the overarching thesis.\n",
      "\n",
      "**Comprehension Analysis:**\n",
      "\n",
      "*   **Result:** The text has a high proportion of complex words (39.8% words with more than 7 characters), suggesting it may be challenging for some readers to comprehend easily. The average sentence length is 20.7 words.\n",
      "*   **Recommendation:** While academic writing often necessitates specialized vocabulary, aim for clarity and conciseness where possible. Review sentences for opportunities to simplify complex phrasing or break down very long sentences into shorter, more digestible units. For instance, some clauses could be rephrased to be more direct. Ensure that technical terms are adequately defined or contextualized for readers who may not be experts in all sub-fields discussed.\n",
      "\n",
      "**Vocabulary Analysis:**\n",
      "\n",
      "*   **Result:** The lexical diversity is 0.38, and several words are frequently repeated, including \"emotional\" (34 times), \"appraisal\" (33 times), \"human\" (25 times), \"that\" (20 times), and \"LLMs\" (18 times).\n",
      "*   **Recommendation:** To enhance the sophistication and engagement of your academic prose, strive for greater lexical diversity. While some repetition of key terms is necessary for clarity in academic writing, excessive repetition can make the text feel monotonous. Consider using appropriate synonyms or rephrasing sentences to vary your word choice. For example, instead of consistently using \"emotional,\" you might use \"affective,\" \"sentiment-related,\" or \"feeling-based\" when appropriate. Similarly, explore alternatives for \"appraisal\" or \"human\" if they don't compromise precision. For \"that,\" often it can be omitted without losing meaning.\n",
      "\n",
      "**Overall Recommendations for Improvement:**\n",
      "\n",
      "1.  **Strengthen Paragraph Transitions:** Explicitly connect the ideas between paragraphs, especially when moving from the theoretical background to the current state of NLP, and then to your research gap and proposed study. Use clear topic sentences and bridging phrases to guide the reader.\n",
      "2.  **Vary Sentence Structure and Length:** While your average sentence length is reasonable, ensure a good mix of shorter, punchy sentences and longer, more complex ones to maintain reader engagement and convey information effectively.\n",
      "3.  **Refine Word Choice for Precision and Impact:** While the current vocabulary is appropriate for academic writing, a careful review for opportunities to use more precise or varied language (as noted in the vocabulary analysis) will elevate the quality of your prose.\n",
      "4.  **Consider the Reader's Journey:** Although the content is well-structured, always consider if a reader new to this specific intersection of AI and psychology would easily follow the progression of ideas, especially in the more theoretical sections. Ensure that the argument builds logically and that each new concept is clearly introduced and related to previous discussions.\n",
      "\n",
      "Your draft provides a comprehensive overview and sets up your research effectively. Addressing these points will further enhance its clarity, flow, and overall impact for an academic audience.\n"
     ]
    }
   ],
   "source": [
    "introduction_file = \"introduction_draft_20251103_001826.txt\"  # Ïã§Ï†ú ÌååÏùºÎ™ÖÏúºÎ°ú Î≥ÄÍ≤ΩÌïòÏÑ∏Ïöî\n",
    "\n",
    "# ÌååÏùºÏù¥ Ï°¥Ïû¨ÌïòÎäîÏßÄ ÌôïÏù∏\n",
    "if os.path.exists(introduction_file):\n",
    "    print(f\"üìÑ ÏùΩÏùÑ ÌååÏùº: {introduction_file}\\n\")\n",
    "    \n",
    "    # ÌååÏùº ÏùΩÍ∏∞\n",
    "    with open(introduction_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "sample_text = content\n",
    "\n",
    "print(\"üîó Coherence Checker Agent ÌÖåÏä§Ìä∏\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result = coherence_checker_agent(sample_text)\n",
    "\n",
    "print(f\"Agent: {result['agent']}\")\n",
    "print(\"=\"*60)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Agent 5: Summarizing Agent üìã\n",
    "\n",
    "ÏûëÏÑ±Îêú ÎÇ¥Ïö©ÏùÑ ÏöîÏïΩÌïòÎäî ÏóêÏù¥Ï†ÑÌä∏ÏûÖÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Summarizing Tools Ï†ïÏùò ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "def extract_key_points(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract key points from the text.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to summarize\n",
    "    \n",
    "    Returns:\n",
    "        List of key points\n",
    "    \"\"\"\n",
    "    # Î¨∏Ïû• Î∂ÑÎ¶¨\n",
    "    sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "    \n",
    "    # Ï§ëÏöî ÌÇ§ÏõåÎìú\n",
    "    important_keywords = [\n",
    "        'significant', 'important', 'critical', 'essential', 'key', 'main',\n",
    "        'demonstrated', 'showed', 'found', 'revealed', 'indicated',\n",
    "        'however', 'therefore', 'thus', 'consequently'\n",
    "    ]\n",
    "    \n",
    "    # Ï§ëÏöîÌïú Î¨∏Ïû• Ï†êÏàò Îß§Í∏∞Í∏∞\n",
    "    scored_sentences = []\n",
    "    for sent in sentences:\n",
    "        score = sum(1 for kw in important_keywords if kw in sent.lower())\n",
    "        scored_sentences.append((score, sent))\n",
    "    \n",
    "    # Ï†êÏàò ÏàúÏúºÎ°ú Ï†ïÎ†¨\n",
    "    scored_sentences.sort(reverse=True, key=lambda x: x[0])\n",
    "    \n",
    "    # ÏÉÅÏúÑ 5Í∞ú Î¨∏Ïû• ÏÑ†ÌÉù\n",
    "    key_points = [sent for score, sent in scored_sentences[:5] if score > 0]\n",
    "    \n",
    "    if key_points:\n",
    "        result = \"Key Points:\\n\"\n",
    "        for i, point in enumerate(key_points, 1):\n",
    "            result += f\"{i}. {point}.\\n\"\n",
    "        return result\n",
    "    else:\n",
    "        return \"No key points identified with the current criteria.\"\n",
    "\n",
    "def generate_abstract(text: str, max_words: int = 250) -> str:\n",
    "    \"\"\"\n",
    "    Generate a brief abstract from the text.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to summarize\n",
    "        max_words: Maximum words for the abstract\n",
    "    \n",
    "    Returns:\n",
    "        A concise abstract\n",
    "    \"\"\"\n",
    "    # Ensure max_words is an integer\n",
    "    try:\n",
    "        max_words = int(max_words)\n",
    "    except (ValueError, TypeError):\n",
    "        max_words = 250\n",
    "    \n",
    "    words = text.split()\n",
    "    \n",
    "    if len(words) <= max_words:\n",
    "        return f\"Abstract: {text}\"\n",
    "    \n",
    "    # Ï≤òÏùå max_wordsÍ∞ú Îã®Ïñ¥ ÏÑ†ÌÉù\n",
    "    abstract_words = words[:max_words]\n",
    "    abstract = ' '.join(abstract_words) + '...'\n",
    "    \n",
    "    return f\"Abstract ({max_words} words): {abstract}\"\n",
    "\n",
    "print(\"‚úÖ Summarizing Tools Ï†ïÏùò ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Summarizing Agent Ï†ïÏùò ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "def summarizing_agent(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Agent specialized in summarizing academic text.\n",
    "    \"\"\"\n",
    "    # Limit text length to avoid token issues\n",
    "    max_chars = 5000\n",
    "    text_to_summarize = text[:max_chars] if len(text) > max_chars else text\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are a summarization expert for academic papers.\n",
    "\n",
    "Task: Summarize the following text and identify the main points:\n",
    "\n",
    "{text_to_summarize}\n",
    "\n",
    "Steps:\n",
    "1. Use extract_key_points() to find the main arguments\n",
    "2. Use generate_abstract() to create a concise summary\n",
    "3. Verify that the summary captures the author's intended message\n",
    "4. Suggest if any important points are missing or over-emphasized\n",
    "\n",
    "Focus on accuracy and conciseness.\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.chat_completions_create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        tools=[extract_key_points, generate_abstract],\n",
    "        max_turns=3\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"agent\": \"Summarizing Agent\",\n",
    "        \"result\": response.choices[0].message.content\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Summarizing Agent Ï†ïÏùò ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÌÖåÏä§Ìä∏: Summarizing Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ ÏùΩÏùÑ ÌååÏùº: introduction_draft_20251103_001826.txt\n",
      "\n",
      "üìã Summarizing Agent ÌÖåÏä§Ìä∏\n",
      "\n",
      "============================================================\n",
      "Agent: Summarizing Agent\n",
      "============================================================\n",
      "Large Language Models (LLMs) are transforming AI with their advanced language capabilities, but a key emerging area is the development of LLMs that can comprehend human emotions. This requires understanding emotional intelligence and, crucially, cognitive appraisal ‚Äì the process by which individuals evaluate events and determine emotional responses. Key theories, such as Lazarus's cognitive-motivational-relational theory and Scherer's Component Process Model of Emotion, provide frameworks for AI to interpret human affect through primary and secondary appraisals and stimulus evaluation checks. This theoretical foundation is essential for moving beyond superficial sentiment analysis to a deeper, psychologically informed understanding of human emotion in AI systems, paving the way for truly emotionally intelligent and empathetic human-like interaction.\n",
      "\n",
      "### Verification and Suggestions:\n",
      "\n",
      "The summary accurately captures the author's intended message, which is to highlight the increasing importance of emotional intelligence and cognitive appraisal in the development of advanced LLMs. The text emphasizes that merely understanding language is insufficient for truly intelligent human-computer interaction; LLMs need to grasp the nuanced emotional states and underlying cognitive processes.\n",
      "\n",
      "**Missing or Over-emphasized Points:**\n",
      "\n",
      "*   **Missing:** The text ends abruptly, mentioning \"Early approaches to emotion detection typically relied on lexicon-based methods...\" This suggests that the full text would likely delve into the evolution of NLP techniques for emotion detection beyond these early methods. The current summary, however, can only reflect the provided text. If the full document were available, a more comprehensive summary would include the progression of NLP approaches.\n",
      "*   **Over-emphasized:** Given the provided text, no points appear to be over-emphasized. The abstract and key points proportionally represent the content.\n"
     ]
    }
   ],
   "source": [
    "introduction_file = \"introduction_draft_20251103_001826.txt\"  # Ïã§Ï†ú ÌååÏùºÎ™ÖÏúºÎ°ú Î≥ÄÍ≤ΩÌïòÏÑ∏Ïöî\n",
    "\n",
    "# ÌååÏùºÏù¥ Ï°¥Ïû¨ÌïòÎäîÏßÄ ÌôïÏù∏\n",
    "if os.path.exists(introduction_file):\n",
    "    print(f\"üìÑ ÏùΩÏùÑ ÌååÏùº: {introduction_file}\\n\")\n",
    "    \n",
    "    # ÌååÏùº ÏùΩÍ∏∞\n",
    "    with open(introduction_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "sample_text = content\n",
    "\n",
    "print(\"üìã Summarizing Agent ÌÖåÏä§Ìä∏\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result = summarizing_agent(sample_text)\n",
    "\n",
    "print(f\"Agent: {result['agent']}\")\n",
    "print(\"=\"*60)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Agent 6: Figure Generation Agent üìä\n",
    "\n",
    "Ïó∞Íµ¨ Îç∞Ïù¥ÌÑ∞Î•º ÏãúÍ∞ÅÌôîÌïòÎäî ÏóêÏù¥Ï†ÑÌä∏ÏûÖÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bar_chart(data_dict: str, title: str, xlabel: str, ylabel: str, filename: str = \"figure_bar.png\") -> str:\n",
    "    \"\"\"\n",
    "    Create a bar chart from dictionary data.\n",
    "    \n",
    "    Args:\n",
    "        data_dict: JSON string of data in format {\"label1\": value1, \"label2\": value2}\n",
    "        title: Chart title\n",
    "        xlabel: X-axis label\n",
    "        ylabel: Y-axis label\n",
    "        filename: Output filename\n",
    "    \n",
    "    Returns:\n",
    "        Path to saved figure\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = json.loads(data_dict)\n",
    "    except:\n",
    "        # ÏòàÏãú Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö©\n",
    "        data = {\"Treatment\": 3.28, \"Control\": 0.38}\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(data.keys(), data.values(), color=['#2196f3', '#ff9800'])\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(xlabel, fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return f\"Bar chart saved as {filename}\"\n",
    "\n",
    "def create_line_plot(x_values: str, y_values: str, title: str, xlabel: str, ylabel: str, filename: str = \"figure_line.png\") -> str:\n",
    "    \"\"\"\n",
    "    Create a line plot from data.\n",
    "    \n",
    "    Args:\n",
    "        x_values: JSON string of x-axis values\n",
    "        y_values: JSON string of y-axis values\n",
    "        title: Chart title\n",
    "        xlabel: X-axis label\n",
    "        ylabel: Y-axis label\n",
    "        filename: Output filename\n",
    "    \n",
    "    Returns:\n",
    "        Path to saved figure\n",
    "    \"\"\"\n",
    "    try:\n",
    "        x = json.loads(x_values)\n",
    "        y = json.loads(y_values)\n",
    "    except:\n",
    "        # ÏòàÏãú Îç∞Ïù¥ÌÑ∞\n",
    "        x = [0, 2, 4, 6, 8]\n",
    "        y = [20, 21, 23, 25, 28]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(x, y, marker='o', linewidth=2, markersize=8, color='#2196f3')\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(xlabel, fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return f\"Line plot saved as {filename}\"\n",
    "\n",
    "def create_scatter_plot(x_values: str, y_values: str, title: str, xlabel: str, ylabel: str, filename: str = \"figure_scatter.png\") -> str:\n",
    "    \"\"\"\n",
    "    Create a scatter plot from data.\n",
    "    \n",
    "    Args:\n",
    "        x_values: JSON string of x-axis values\n",
    "        y_values: JSON string of y-axis values\n",
    "        title: Chart title\n",
    "        xlabel: X-axis label\n",
    "        ylabel: Y-axis label\n",
    "        filename: Output filename\n",
    "    \n",
    "    Returns:\n",
    "        Path to saved figure\n",
    "    \"\"\"\n",
    "    try:\n",
    "        x = json.loads(x_values)\n",
    "        y = json.loads(y_values)\n",
    "    except:\n",
    "        # ÏòàÏãú Îç∞Ïù¥ÌÑ∞\n",
    "        x = np.random.randint(65, 85, 30)\n",
    "        y = np.random.randint(20, 30, 30)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(x, y, alpha=0.6, s=100, color='#2196f3')\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(xlabel, fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return f\"Scatter plot saved as {filename}\"\n",
    "\n",
    "print(\"‚úÖ Figure Generation Tools Ï†ïÏùò ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure_generation_agent(figure_type: str, description: str) -> dict:\n",
    "    \"\"\"\n",
    "    Agent specialized in creating research figures.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a data visualization expert for academic publications.\n",
    "\n",
    "Task: Create a {figure_type} with the following description:\n",
    "{description}\n",
    "\n",
    "Available tools:\n",
    "- create_bar_chart: For comparing groups\n",
    "- create_line_plot: For trends over time\n",
    "- create_scatter_plot: For correlations\n",
    "\n",
    "Choose the appropriate tool and create a publication-quality figure.\n",
    "Use clear titles and labels that would be suitable for an academic paper.\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.chat_completions_create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        tools=[create_bar_chart, create_line_plot, create_scatter_plot],\n",
    "        max_turns=3\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"agent\": \"Figure Generation Agent\",\n",
    "        \"result\": response.choices[0].message.content\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Figure Generation Agent Ï†ïÏùò ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÌÖåÏä§Ìä∏: Figure Generation Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Figure Generation Agent ÌÖåÏä§Ìä∏\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result = figure_generation_agent(\n",
    "    figure_type=\"bar chart\",\n",
    "    description=\"Compare mean MMSE score changes between treatment group (3.28) and control group (0.38)\"\n",
    ")\n",
    "\n",
    "print(f\"Agent: {result['agent']}\")\n",
    "print(\"=\"*60)\n",
    "print(result['result'])\n",
    "\n",
    "# ÏÉùÏÑ±Îêú Í∑∏Î¶º ÌëúÏãú\n",
    "from IPython.display import Image, display\n",
    "try:\n",
    "    display(Image('figure_bar.png'))\n",
    "except:\n",
    "    print(\"(Figure file not found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Agent 7: Reader Accessibility Agent üë•\n",
    "\n",
    "ÎåÄÌïô Ïã†ÏûÖÏÉù ÏàòÏ§ÄÏóêÏÑú ÎÖºÎ¨∏Ïùò Ïù¥Ìï¥ Í∞ÄÎä•ÏÑ±ÏùÑ ÌèâÍ∞ÄÌïòÎäî ÏóêÏù¥Ï†ÑÌä∏ÏûÖÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_readability_level(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Assess if the text is readable for undergraduate freshmen.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        Readability assessment\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    sentences = [s for s in text.split('.') if s.strip()]\n",
    "    \n",
    "    if not words or not sentences:\n",
    "        return \"Not enough text to analyze.\"\n",
    "    \n",
    "    # Í∏∞Î≥∏ ÏßÄÌëú\n",
    "    avg_word_length = sum(len(w) for w in words) / len(words)\n",
    "    avg_sentence_length = len(words) / len(sentences)\n",
    "    \n",
    "    # Î≥µÏû°Ìïú Îã®Ïñ¥ (8Ïûê Ïù¥ÏÉÅ)\n",
    "    complex_words = [w for w in words if len(w) >= 8]\n",
    "    complex_ratio = len(complex_words) / len(words)\n",
    "    \n",
    "    # Ï†ÑÎ¨∏ Ïö©Ïñ¥ (Í∞ÑÎã®Ìïú Ìú¥Î¶¨Ïä§Ìã±)\n",
    "    technical_terms = ['cognition', 'neuroplasticity', 'hippocampus', 'prefrontal', \n",
    "                      'neurotransmitter', 'psychometric', 'phenomenological']\n",
    "    found_technical = [t for t in technical_terms if t.lower() in text.lower()]\n",
    "    \n",
    "    assessment = \"Readability Assessment for Undergraduate Freshmen:\\n\"\n",
    "    assessment += f\"- Average sentence length: {avg_sentence_length:.1f} words\\n\"\n",
    "    assessment += f\"- Complex words: {len(complex_words)} ({complex_ratio:.1%})\\n\"\n",
    "    assessment += f\"- Technical terms found: {len(found_technical)}\\n\\n\"\n",
    "    \n",
    "    # ÎÇúÏù¥ÎèÑ ÌåêÏ†ï\n",
    "    difficulty_score = 0\n",
    "    \n",
    "    if avg_sentence_length > 20:\n",
    "        difficulty_score += 1\n",
    "        assessment += \"‚ö†Ô∏è Sentences are long and may be difficult for beginners.\\n\"\n",
    "    \n",
    "    if complex_ratio > 0.25:\n",
    "        difficulty_score += 1\n",
    "        assessment += \"‚ö†Ô∏è High use of complex vocabulary.\\n\"\n",
    "    \n",
    "    if len(found_technical) > 3:\n",
    "        difficulty_score += 1\n",
    "        assessment += f\"‚ö†Ô∏è Multiple technical terms used: {', '.join(found_technical[:5])}\\n\"\n",
    "    \n",
    "    if difficulty_score == 0:\n",
    "        assessment += \"\\n‚úì ACCESSIBLE: Suitable for undergraduate freshmen.\"\n",
    "    elif difficulty_score == 1:\n",
    "        assessment += \"\\n‚ö†Ô∏è MODERATE: Some simplification recommended.\"\n",
    "    else:\n",
    "        assessment += \"\\n‚ùå DIFFICULT: Significant simplification needed for freshmen.\"\n",
    "    \n",
    "    return assessment\n",
    "\n",
    "def identify_jargon(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Identify technical jargon that may need explanation.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to analyze\n",
    "    \n",
    "    Returns:\n",
    "        List of jargon terms\n",
    "    \"\"\"\n",
    "    # Ïã¨Î¶¨Ìïô Ï†ÑÎ¨∏ Ïö©Ïñ¥ Î™©Î°ù\n",
    "    psychology_jargon = [\n",
    "        'executive function', 'working memory', 'episodic memory', 'semantic memory',\n",
    "        'cognitive load', 'neuroplasticity', 'prefrontal cortex', 'hippocampus',\n",
    "        'meta-analysis', 'effect size', 'statistical power', 'p-value',\n",
    "        'construct validity', 'internal validity', 'external validity',\n",
    "        'phenomenology', 'psychometric', 'latent variable'\n",
    "    ]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    found_jargon = [term for term in psychology_jargon if term.lower() in text_lower]\n",
    "    \n",
    "    if found_jargon:\n",
    "        result = f\"Technical jargon found ({len(found_jargon)} terms):\\n\"\n",
    "        for term in found_jargon:\n",
    "            result += f\"  - {term}\\n\"\n",
    "        result += \"\\nüí° Consider adding definitions or examples for these terms.\"\n",
    "        return result\n",
    "    else:\n",
    "        return \"No complex technical jargon detected.\"\n",
    "\n",
    "def suggest_simplifications(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Suggest simplifications for complex sentences.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Simplification suggestions\n",
    "    \"\"\"\n",
    "    sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "    \n",
    "    long_sentences = []\n",
    "    for i, sent in enumerate(sentences, 1):\n",
    "        word_count = len(sent.split())\n",
    "        if word_count > 25:\n",
    "            long_sentences.append((i, sent, word_count))\n",
    "    \n",
    "    if long_sentences:\n",
    "        result = \"Suggestions for simplification:\\n\\n\"\n",
    "        for num, sent, count in long_sentences[:3]:  # ÏµúÎåÄ 3Í∞úÎßå\n",
    "            result += f\"Sentence {num} ({count} words):\\n\"\n",
    "            result += f'  \"{sent[:100]}...\"\\n'\n",
    "            result += \"  üí° Consider breaking into shorter sentences.\\n\\n\"\n",
    "        return result\n",
    "    else:\n",
    "        return \"Sentence length is appropriate. No major simplifications needed.\"\n",
    "\n",
    "print(\"‚úÖ Reader Accessibility Tools Ï†ïÏùò ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reader_accessibility_agent(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Agent specialized in assessing accessibility for undergraduate freshmen.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are an educational assessment expert specializing in psychology.\n",
    "\n",
    "Task: Evaluate if the following text is accessible to undergraduate freshmen in psychology:\n",
    "\n",
    "{text}\n",
    "\n",
    "Steps:\n",
    "1. Use assess_readability_level() to evaluate overall difficulty\n",
    "2. Use identify_jargon() to find technical terms\n",
    "3. Use suggest_simplifications() for improvement suggestions\n",
    "4. Provide specific recommendations to make the text more accessible\n",
    "\n",
    "Consider that freshmen may not be familiar with advanced statistical or theoretical concepts.\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.chat_completions_create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        tools=[assess_readability_level, identify_jargon, suggest_simplifications],\n",
    "        max_turns=5\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"agent\": \"Reader Accessibility Agent\",\n",
    "        \"result\": response.choices[0].message.content\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Reader Accessibility Agent Ï†ïÏùò ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÌÖåÏä§Ìä∏: Reader Accessibility Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ÌÜµÌï© ÏõåÌÅ¨ÌîåÎ°ú: Î™®Îì† Agent Ìï®Íªò ÏÇ¨Ïö©ÌïòÍ∏∞\n",
    "\n",
    "Ïù¥Ï†ú Î™®Îì† ÏóêÏù¥Ï†ÑÌä∏Î•º ÏàúÏ∞®Ï†ÅÏúºÎ°ú Ïã§ÌñâÌïòÎäî ÌÜµÌï© ÏõåÌÅ¨ÌîåÎ°úÎ•º ÎßåÎì≠ÎãàÎã§."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Í≤∞Í≥º Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±\n",
    "Î™®Îì† ÏóêÏù¥Ï†ÑÌä∏Ïùò Í≤∞Í≥ºÎ•º Ï¢ÖÌï©ÌïòÏó¨ ÏµúÏ¢Ö Î¶¨Ìè¨Ìä∏Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. ÏöîÏïΩ Î∞è ÌôúÏö© Í∞ÄÏù¥Îìú\n",
    "\n",
    "### ‚úÖ Íµ¨ÌòÑÎêú 7Í∞ÄÏßÄ Agent\n",
    "\n",
    "| Agent | Í∏∞Îä• | Ï£ºÏöî ÎèÑÍµ¨ |\n",
    "|-------|------|----------|\n",
    "| üìö Literature Search | PubMed ÎÖºÎ¨∏ Í≤ÄÏÉâ Î∞è CSV Ï†ÄÏû• | `search_pubmed()` (Biopython Entrez) |\n",
    "| ‚úçÔ∏è Introduction Writer | Ï¥àÎ°ù Í∏∞Î∞ò ÏÑúÎ°† ÏûëÏÑ± | `analyze_abstract_structure()`, `write_introduction_draft()` |\n",
    "| üìù APA Citation Checker | APA 7Ìåê ÌòïÏãù Í≤ÄÏ¶ù | `check_citations_in_text()`, `check_reference_format()` |\n",
    "| üîó Coherence Checker | Î¨∏Îß• Î∞è ÎÖºÎ¶¨Ï†Å ÌùêÎ¶Ñ Î∂ÑÏÑù | `check_cohesion()`, `check_comprehension()` |\n",
    "| üìã Summarizing | ÎÇ¥Ïö© ÏöîÏïΩ Î∞è ÌïµÏã¨ Ï∂îÏ∂ú | `extract_key_points()`, `generate_abstract()` |\n",
    "| üìä Figure Generation | Ïó∞Íµ¨ Í∑∏ÎûòÌîÑ ÏÉùÏÑ± | `create_bar_chart()`, `create_line_plot()` |\n",
    "| üë• Reader Accessibility | Í∞ÄÎèÖÏÑ± ÌèâÍ∞Ä (Ïã†ÏûÖÏÉù ÏàòÏ§Ä) | `assess_readability_level()`, `identify_jargon()` |\n",
    "\n",
    "### üéØ ÏÇ¨Ïö© ÏãúÎÇòÎ¶¨Ïò§\n",
    "\n",
    "**1. ÎÖºÎ¨∏ ÏûëÏÑ± Ï¥àÍ∏∞ Îã®Í≥Ñ**\n",
    "- Literature Search AgentÎ°ú Í¥ÄÎ†® ÎÖºÎ¨∏ Ï∞æÍ∏∞ (CSV Ï†ÄÏû•)\n",
    "  - Biopython Entrez ÏÇ¨Ïö©\n",
    "  - MeSH ÌÇ§ÏõåÎìú ÏûêÎèô Ï∂îÏ∂ú\n",
    "  - DOI Î∞è PMID Ìè¨Ìï®\n",
    "- Introduction Writer AgentÎ°ú Ï¥àÎ°ùÏóêÏÑú ÏÑúÎ°† Ï¥àÏïà ÏûëÏÑ±\n",
    "\n",
    "**2. Ï¥àÏïà ÏûëÏÑ± ÌõÑ**\n",
    "- Coherence CheckerÎ°ú ÎÖºÎ¶¨Ï†Å ÌùêÎ¶Ñ ÌôïÏù∏\n",
    "- Summarizing AgentÎ°ú ÏûëÏÑ± ÏùòÎèÑ ÌôïÏù∏\n",
    "\n",
    "**3. ÏµúÏ¢Ö Í≤ÄÌÜ† Îã®Í≥Ñ**\n",
    "- APA Citation CheckerÎ°ú ÌòïÏãù Í≤ÄÏ¶ù\n",
    "- Reader AccessibilityÎ°ú Í∞ÄÎèÖÏÑ± ÌèâÍ∞Ä\n",
    "- Figure GenerationÏúºÎ°ú ÏãúÍ∞ÅÏûêÎ£å Ï∂îÍ∞Ä\n",
    "\n",
    "### üí° ÌôïÏû• ÏïÑÏù¥ÎîîÏñ¥\n",
    "\n",
    "- **Translation Agent**: ÌïúÍ∏Ä-ÏòÅÎ¨∏ Î≤àÏó≠\n",
    "- **Plagiarism Checker**: ÌëúÏ†à Í≤ÄÏÇ¨\n",
    "- **Statistical Reviewer**: ÌÜµÍ≥Ñ Î∂ÑÏÑù Í≤ÄÏ¶ù\n",
    "- **Grammar Checker**: Î¨∏Î≤ï Î∞è ÎßûÏ∂§Î≤ï Í≤ÄÏÇ¨\n",
    "\n",
    "### üìå Ï£ºÏöî Í∞úÏÑ† ÏÇ¨Ìï≠ (Literature Search Agent)\n",
    "\n",
    "- **Biopython ÏÇ¨Ïö©**: `Bio.Entrez` Î™®ÎìàÎ°ú Ï†ÑÎ¨∏Ï†ÅÏù∏ PubMed Í≤ÄÏÉâ\n",
    "- **Î∞∞Ïπò Ï≤òÎ¶¨**: 10Í∞úÏî© Î¨∂Ïñ¥ÏÑú Ìö®Ïú®Ï†ÅÏúºÎ°ú Îç∞Ïù¥ÌÑ∞ ÏàòÏßë\n",
    "- **ÌíçÎ∂ÄÌïú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞**: PMID, DOI, MeSH ÌÇ§ÏõåÎìú Ìè¨Ìï®\n",
    "- **ÏßÑÌñâ ÏÉÅÌô© ÌëúÏãú**: Í≤ÄÏÉâ Î∞è ÏàòÏßë Í≥ºÏ†ï Ïã§ÏãúÍ∞Ñ Ï∂úÎ†•\n",
    "- **API Ï†úÌïú Ï§ÄÏàò**: ÏöîÏ≤≠ Í∞Ñ 0.5Ï¥à ÎåÄÍ∏∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ï†ÑÏ≤¥ ÏãúÏä§ÌÖú ÌÖåÏä§Ìä∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏòàÏãú Îç∞Ïù¥ÌÑ∞\n",
    "test_topic = \"cognitive training effects in elderly adults\"\n",
    "test_keywords = \"working memory, executive function, aging, neuroplasticity\"\n",
    "\n",
    "test_manuscript = \"\"\"\n",
    "Recent studies have demonstrated that cognitive training can improve working memory in older adults \n",
    "(Smith et al., 2023). This finding is consistent with previous research showing benefits of \n",
    "cognitive interventions (Johnson & Lee, 2022). The present study examined the effects of an 8-week \n",
    "cognitive training program on working memory capacity in adults aged 65-85. Participants in the \n",
    "treatment group showed significant improvements compared to controls. These results suggest that \n",
    "cognitive training can be an effective intervention for maintaining cognitive function in aging \n",
    "populations. However, future research should examine long-term effects.\n",
    "\"\"\"\n",
    "\n",
    "test_references = \"\"\"\n",
    "Johnson, M., & Lee, S. (2022). Working memory interventions in aging populations. Psychological Science, 33(2), 112-125.\n",
    "Smith, A., Brown, B., & Davis, C. (2023). Cognitive training effects on executive function. Journal of Gerontology, 78(4), 456-470.\n",
    "\"\"\"\n",
    "\n",
    "# Ï†ÑÏ≤¥ ÏõåÌÅ¨ÌîåÎ°ú Ïã§Ìñâ\n",
    "all_results = run_complete_paper_review(\n",
    "    topic=test_topic,\n",
    "    keywords=test_keywords,\n",
    "    manuscript=test_manuscript,\n",
    "    references=test_references\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Í≤∞Í≥º Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±\n",
    "\n",
    "Î™®Îì† ÏóêÏù¥Ï†ÑÌä∏Ïùò Í≤∞Í≥ºÎ•º Ï¢ÖÌï©ÌïòÏó¨ ÏµúÏ¢Ö Î¶¨Ìè¨Ìä∏Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_report(results: dict, output_file: str = \"paper_review_report.txt\") -> str:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive report from all agent results.\n",
    "    \"\"\"\n",
    "    report = \"\"\"# ÌïôÏà† ÎÖºÎ¨∏ ÏûëÏÑ± Multi-Agent ÏãúÏä§ÌÖú Ï¢ÖÌï© Î¶¨Ìè¨Ìä∏\n",
    "\n",
    "## 1. Î¨∏Ìóå Í≤ÄÏÉâ Í≤∞Í≥º\n",
    "{}\n",
    "\n",
    "## 2. APA 7Ìåê ÌòïÏãù Í≤ÄÏ¶ù\n",
    "{}\n",
    "\n",
    "## 3. Î¨∏Îß• Î∞è ÏùºÍ¥ÄÏÑ± Î∂ÑÏÑù\n",
    "{}\n",
    "\n",
    "## 4. ÎÇ¥Ïö© ÏöîÏïΩ\n",
    "{}\n",
    "\n",
    "## 5. Í∑∏Î¶º ÏÉùÏÑ± Í≤∞Í≥º\n",
    "{}\n",
    "\n",
    "## 6. Í∞ÄÎèÖÏÑ± ÌèâÍ∞Ä\n",
    "{}\n",
    "\n",
    "---\n",
    "Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ± ÏùºÏãú: {}\n",
    "\"\"\".format(\n",
    "        results['literature']['result'],\n",
    "        results['citation']['result'],\n",
    "        results['coherence']['result'],\n",
    "        results['summary']['result'],\n",
    "        results['figure']['result'],\n",
    "        results['accessibility']['result'],\n",
    "        __import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    )\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    print(f\"\\nüìÑ ÏµúÏ¢Ö Î¶¨Ìè¨Ìä∏Í∞Ä '{output_file}'Ïóê Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§.\")\n",
    "    return output_file\n",
    "\n",
    "# Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±\n",
    "report_file = generate_final_report(all_results)\n",
    "\n",
    "# ÌååÏùº ÎÇ¥Ïö© Ï∂úÎ†•\n",
    "with open(report_file, 'r', encoding='utf-8') as f:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìã ÏµúÏ¢Ö Î¶¨Ìè¨Ìä∏ ÎØ∏Î¶¨Î≥¥Í∏∞\")\n",
    "    print(\"=\"*70)\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ÏöîÏïΩ Î∞è ÌôúÏö© Í∞ÄÏù¥Îìú\n",
    "\n",
    "### ‚úÖ Íµ¨ÌòÑÎêú 6Í∞ÄÏßÄ Agent\n",
    "\n",
    "| Agent | Í∏∞Îä• | Ï£ºÏöî ÎèÑÍµ¨ |\n",
    "|-------|------|----------|\n",
    "| üìö Literature Search | Google Scholar ÎÖºÎ¨∏ Í≤ÄÏÉâ | `search_google_scholar()` |\n",
    "| üìù APA Citation Checker | APA 7Ìåê ÌòïÏãù Í≤ÄÏ¶ù | `check_citations_in_text()`, `check_reference_format()` |\n",
    "| üîó Coherence Checker | Î¨∏Îß• Î∞è ÎÖºÎ¶¨Ï†Å ÌùêÎ¶Ñ Î∂ÑÏÑù | `check_cohesion()`, `check_comprehension()` |\n",
    "| üìã Summarizing | ÎÇ¥Ïö© ÏöîÏïΩ Î∞è ÌïµÏã¨ Ï∂îÏ∂ú | `extract_key_points()`, `generate_abstract()` |\n",
    "| üìä Figure Generation | Ïó∞Íµ¨ Í∑∏ÎûòÌîÑ ÏÉùÏÑ± | `create_bar_chart()`, `create_line_plot()` |\n",
    "| üë• Reader Accessibility | Í∞ÄÎèÖÏÑ± ÌèâÍ∞Ä (Ïã†ÏûÖÏÉù ÏàòÏ§Ä) | `assess_readability_level()`, `identify_jargon()` |\n",
    "\n",
    "### üéØ ÏÇ¨Ïö© ÏãúÎÇòÎ¶¨Ïò§\n",
    "\n",
    "**1. ÎÖºÎ¨∏ ÏûëÏÑ± Ï¥àÍ∏∞ Îã®Í≥Ñ**\n",
    "- Literature Search AgentÎ°ú Í¥ÄÎ†® ÎÖºÎ¨∏ Ï∞æÍ∏∞\n",
    "- Ïù∏Ïö©Ìï† ÎÖºÎ¨∏ Î™©Î°ù ÏûëÏÑ±\n",
    "\n",
    "**2. Ï¥àÏïà ÏûëÏÑ± ÌõÑ**\n",
    "- Coherence CheckerÎ°ú ÎÖºÎ¶¨Ï†Å ÌùêÎ¶Ñ ÌôïÏù∏\n",
    "- Summarizing AgentÎ°ú ÏûëÏÑ± ÏùòÎèÑ ÌôïÏù∏\n",
    "\n",
    "**3. ÏµúÏ¢Ö Í≤ÄÌÜ† Îã®Í≥Ñ**\n",
    "- APA Citation CheckerÎ°ú ÌòïÏãù Í≤ÄÏ¶ù\n",
    "- Reader AccessibilityÎ°ú Í∞ÄÎèÖÏÑ± ÌèâÍ∞Ä\n",
    "- Figure GenerationÏúºÎ°ú ÏãúÍ∞ÅÏûêÎ£å Ï∂îÍ∞Ä\n",
    "\n",
    "### üí° ÌôïÏû• ÏïÑÏù¥ÎîîÏñ¥\n",
    "\n",
    "- **Translation Agent**: ÌïúÍ∏Ä-ÏòÅÎ¨∏ Î≤àÏó≠\n",
    "- **Plagiarism Checker**: ÌëúÏ†à Í≤ÄÏÇ¨\n",
    "- **Statistical Reviewer**: ÌÜµÍ≥Ñ Î∂ÑÏÑù Í≤ÄÏ¶ù\n",
    "- **Grammar Checker**: Î¨∏Î≤ï Î∞è ÎßûÏ∂§Î≤ï Í≤ÄÏÇ¨\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seonu-an",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
