# Peer Feedback Session Design Prompt

## Context
You are a world-renowned expert in science writing pedagogy with 20+ years of experience teaching graduate students how to write for top-tier journals (Nature, Science, Cell, PNAS). You have designed and refined peer review workshops for psychology, neuroscience, and biology graduate programs at leading universities (MIT, Stanford, Cambridge, Harvard).

## Task
Design a comprehensive peer feedback session for a 90-minute workshop where psychology graduate students (석사/박사) bring their own research abstracts and receive structured peer evaluation to improve them to Nature/Science publication level.

## Constraints

### Time Structure (90 minutes total)
- **15 minutes**: Lecture on Nature/Science abstract characteristics
- **70 minutes**: Hands-on workshop with peer feedback (THIS IS YOUR FOCUS)
- **5 minutes**: Wrap-up and assignment

### Student Profile
- Psychology graduate students (6-12 students per class)
- Conducting their own research (thesis/dissertation projects)
- Already have draft abstracts written before class
- Varying English proficiency levels (provide Korean support)
- Familiar with basic AI tools (ChatGPT, Claude)

### Learning Objectives
By the end of the session, students should be able to:
1. Identify which of 4 opening patterns their abstract uses (Problem/Gap/Opportunity/Challenge-driven)
2. Evaluate whether significance framing is broad (cross-disciplinary/societal) or narrow (single discipline)
3. Distinguish quantitative results (percentages, fold changes) from vague claims
4. Recognize explicit novelty statements vs implicit contributions
5. Provide actionable, specific feedback to peers using top-tier journal editor criteria
6. Revise their own abstracts based on peer feedback and AI assistance

## Required Deliverables

### 1. Detailed 70-Minute Hands-On Workshop Structure
Break down the 70 minutes into specific activities with:
- Exact time allocations (e.g., "Minutes 0-20: Exemplar Analysis")
- Activity names and descriptions
- Instructor actions during each segment
- Student actions during each segment
- Transition strategies between activities
- Materials needed for each activity

**Pedagogical Requirements:**
- Include exemplar analysis (students learn from 2 Nature/Science abstract pairs BEFORE evaluating peers)
- Structure peer feedback in small groups (3-4 students per group for sufficient feedback)
- Allow time for AI-enhanced revision using peer feedback
- Include a validation round where improvements are acknowledged
- Balance individual work, small group discussion, and whole class synthesis

### 2. Peer Review Rubric (5 Dimensions)
Create a detailed rubric with:
- **5 evaluation dimensions** aligned with Week 2 learning objectives:
  1. Opening Pattern (Problem/Gap/Opportunity/Challenge-driven)
  2. Broad Significance (cross-disciplinary impact vs narrow scope)
  3. Quantitative Results (specific numbers vs vague terms)
  4. Explicit Novelty (what's new is stated clearly)
  5. Logical Structure (coherent flow from problem to contribution)
- **1-5 scale for each dimension** with clear descriptors:
  - 1 = Major revision needed (Nature editor would desk reject)
  - 2 = Substantial improvement needed
  - 3 = Adequate but not top-tier level
  - 4 = Strong, approaching Nature/Science level
  - 5 = Exceptional, ready for Nature/Science submission
- **Concrete examples** for each score level in each dimension
- **Korean translation** for all dimensions and descriptors

### 3. Editor Protocol Template
Design a "Top-Tier Journal Editor Protocol" that students use to give feedback:
- **Desk Reject or Send to Review?** (Yes/No decision with 1-sentence justification)
- **Strongest Element** (which of 5 dimensions is publication-ready)
- **Fatal Flaw** (which dimension would cause rejection if not fixed)
- **One Concrete Revision** (specific sentence-level rewrite suggestion, NOT vague advice like "improve clarity")

Include:
- Completed example showing the protocol applied to a real abstract
- Instructions on how to use the protocol (what makes feedback "concrete" vs "vague")
- Korean translation

### 4. Notion Workspace Design
Specify the structure for Notion-based peer review:

**Pre-Class Setup (2 days before):**
- How students submit abstracts
- How instructor organizes small groups (3-4 students)
- Database properties needed

**Template Button Structure:**
Each student's "Peer Review Workspace" page should include:
- Section for their abstract (paste original text)
- 5-dimension rubric table (self-assessment before peer review)
- Editor Protocol sections (one per reviewer, 2-3 reviewers per student)
- AI Revision section (paste improved version after using ChatGPT/Claude)
- Reflection section (what changed and why)

Provide:
- Exact Notion block structure (headings, callouts, tables, toggle blocks)
- Template Button content (what gets duplicated when student clicks)
- Group linking strategy (how to connect 3-4 students' pages together)

### 5. Small Group Facilitation Guide
Design the peer review process for groups of 3-4:

**Round 1 (25 minutes):**
- How to allocate time so each student gets 6-7 minutes
- Procedure: Author reads abstract aloud (2 min) → Peer reviewers use Editor Protocol (4-5 min) → Brief discussion (1 min)
- Instructor role: How to circulate, when to intervene, what to monitor
- Common pitfalls and how to avoid them (e.g., too vague feedback, too harsh criticism)

**Round 2 (10 minutes - validation):**
- How students share revised versions
- What peers should look for (did revisions address fatal flaw?)
- Quick feedback format (30 seconds per person)

### 6. AI Integration Strategy
Specify how students use AI during the 15-minute revision window:

**Effective Prompts:**
- Template: "I received this peer feedback: [paste Editor Protocol feedback]. My abstract currently says: [paste abstract]. Rewrite the [specific section] to address the fatal flaw of [specific issue]. Maintain scientific accuracy and use this opening pattern: [Pattern name]."
- 3-5 example prompts for common revision needs

**AI Usage Guidelines:**
- What AI can help with (alternative phrasings, significance framing, quantitative emphasis)
- What AI cannot do (fabricate results, change core science)
- How to evaluate AI output (does it address peer feedback? is it still accurate?)

### 7. Instructor Facilitation Scripts
Provide word-for-word scripts for key moments:

**Opening (1 minute):**
"Today you'll experience what Nature editors do when evaluating abstracts. You'll use the same criteria they use to desk-reject 80% of submissions..."

**Exemplar Analysis Introduction (1 minute):**
"Before evaluating each other, let's calibrate our standards by analyzing two published Nature abstracts..."

**Transition to Peer Review (1 minute):**
"You've now derived a checklist from exemplars. Time to apply it to real abstracts - yours. Remember: honest feedback is kind feedback..."

**AI Revision Introduction (1 minute):**
"You've received expert feedback. Now let's use AI to rapidly test different revisions. Here's how to prompt effectively..."

**Wrap-up (3 minutes):**
"What did you learn about your abstract? What patterns emerged across the group? Next week, you'll apply this same rigor to Introduction sections..."

### 8. Assessment and Accountability
Design mechanisms to ensure quality feedback:

**Peer Feedback Quality Checklist:**
Students self-assess their own feedback:
- [ ] Used all 5 rubric dimensions
- [ ] Provided 1-5 scores with justification
- [ ] Gave ONE concrete revision (specific rewrite, not vague advice)
- [ ] Identified fatal flaw clearly
- [ ] Completed feedback in respectful, professional tone

**Instructor Monitoring:**
- Real-time checklist of what to watch during workshop
- Red flags requiring intervention
- How to provide corrective feedback during workshop
- Post-workshop review process (spot-check feedback quality)

### 9. Example Materials
Include concrete examples:

**Example Abstract with Full Annotation:**
- One psychology research abstract (250 words)
- Annotated with all 5 dimensions marked
- Completed rubric showing scores
- Completed Editor Protocol showing feedback
- Revised version after AI enhancement

**Common Failure Modes:**
- 3-5 examples of BAD peer feedback (too vague, too harsh, not actionable)
- Corrected versions showing GOOD feedback
- Explanation of differences

### 10. Cultural and Language Considerations
Address challenges for Korean psychology graduate students:

**Korean Academic Writing Culture:**
- Traditional Korean academic writing tends toward humility and understatement
- Nature/Science requires bold claims and explicit significance
- How to help students overcome cultural hesitation

**English Language Support:**
- Common grammar/phrasing issues in Korean writers' abstracts
- AI prompt templates in Korean for students less comfortable with English
- Bilingual rubric and protocols

**Psychological Safety:**
- How to create supportive environment where students feel safe sharing drafts
- Emphasize "we're all learning together" vs "judgment"
- Frame peer review as collective problem-solving, not criticism

## Output Format

Structure your response as a comprehensive implementation guide with these sections:

1. **Executive Summary** (200 words): Overview of pedagogical approach and key innovations
2. **70-Minute Workshop Timeline** (detailed breakdown of all activities)
3. **Peer Review Rubric** (full 5-dimension rubric with examples)
4. **Editor Protocol Template** (with completed example)
5. **Notion Workspace Specification** (detailed structure)
6. **Small Group Facilitation Guide** (procedures and scripts)
7. **AI Integration Strategy** (prompts and guidelines)
8. **Instructor Scripts** (word-for-word for key transitions)
9. **Assessment Mechanisms** (quality checklists)
10. **Example Materials** (annotated abstract, feedback samples)
11. **Cultural Considerations** (Korean context adaptations)
12. **Appendices**:
    - Appendix A: Pre-class email to students
    - Appendix B: Small group assignment strategy
    - Appendix C: Troubleshooting guide (common problems and solutions)

## Quality Standards

Your design should reflect:
- **Evidence-based pedagogy**: Cite specific learning science principles (e.g., deliberate practice, cognitive apprenticeship, scaffolding)
- **Actionable specificity**: Every instruction should be concrete enough that a new instructor could implement without guessing
- **Time realism**: 70 minutes is tight - activities must be timed precisely
- **Graduate-level rigor**: Avoid undergraduate-style activities; treat students as junior colleagues
- **Cultural sensitivity**: Acknowledge Korean academic culture without stereotyping
- **Technology integration**: Leverage Notion and AI thoughtfully, not as gimmicks

## Success Metrics

A successful design will enable:
- 90%+ of students to provide feedback scoring 4+ on specificity and actionability
- 80%+ of revised abstracts showing measurable improvement (e.g., rubric score increase of 3+ points)
- Students reporting increased confidence in identifying Nature/Science-level characteristics
- Instructor able to facilitate without extensive prior training

---

**Now design this peer feedback session following all requirements above.**
